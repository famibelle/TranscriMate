import asyncio
import io
import json
import logging
import os
import subprocess
import tempfile
import time
from collections import deque
from contextlib import asynccontextmanager
from typing import Generator, Tuple

from temp_manager import TempFileManager, async_temp_manager_context, async_temp_file_context

import filetype
import torch
from dotenv import load_dotenv
from fastapi import FastAPI, File, HTTPException, UploadFile, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from moviepy.editor import VideoFileClip
from openai import OpenAI
from pyannote.audio import Pipeline
from pyannote.audio.pipelines.utils.hook import ProgressHook
from pydantic import BaseModel, StrictStr
from pydub import AudioSegment
from RAG import setup_rag_pipeline
# from silero_vad import get_speech_timestamps
# from pysilero_vad import SileroVoiceActivityDetector
from silero_vad import get_speech_timestamps, load_silero_vad, read_audio
from starlette.websockets import WebSocketDisconnect

# Configuration du logging avec plus de verbosit√©
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Affichage dans la console
    ]
)

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

logging.info(f"TF32 matmul: {torch.backends.cuda.matmul.allow_tf32}")
logging.info(f"TF32 cuDNN: {torch.backends.cudnn.allow_tf32}")

import gc
import threading
import warnings

from pydub import AudioSegment
from transformers import pipeline

warnings.filterwarnings("ignore", category=FutureWarning)

load_dotenv()

# Configuration cross-platform des r√©pertoires
TEMP_FOLDER = tempfile.gettempdir()
HF_cache = '/mnt/.cache/' if os.name != 'nt' else os.path.join(os.path.expanduser('~'), '.cache', 'huggingface')
Model_dir = '/mnt/Models' if os.name != 'nt' else os.path.join(os.path.expanduser('~'), 'Models')

HUGGING_FACE_KEY = os.environ.get("HuggingFace_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY_MCF")

server_url = os.getenv("SERVER_URL")

# Configuration cross-platform des r√©pertoires
if os.name != 'nt':  # Unix/Linux
    directories = ["/mnt/Models", "/mnt/logs", "/mnt/.cache"]
    for directory in directories:
        if not os.path.exists(directory):
            os.system(f"sudo mkdir -p {directory}")
    os.system("sudo chmod -R 755 /mnt/Models /mnt/.cache /mnt/logs")
    os.system("sudo chown -R $USER:$USER /mnt/Models /mnt/.cache /mnt/logs")
else:  # Windows
    directories = [HF_cache, Model_dir, os.path.join(os.path.expanduser('~'), 'logs')]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

os.environ['HF_HOME'] = HF_cache
os.environ['TRANSFORMERS_CACHE'] = HF_cache

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    logging.info(f"GPU trouv√©! on utilise CUDA. Device: {device}")

else:
    logging.info(f"Pas de GPU de disponible... Device: {device}")




# V√©rifier le type par d√©faut de tensor
print(f"Type de donn√©e par d√©faut : {torch.get_default_dtype()}")

# V√©rifier la pr√©cision pour les calculs en FP16
print(f"Disponibilit√© de la pr√©cision FP16 : {torch.cuda.is_available() and torch.cuda.get_device_capability() >= (8, 0)}")

# Initialisation de `current_settings` avec des valeurs par d√©faut
# global current_settings
current_settings = {
    "task": "transcribe",
    "model": "openai/whisper-large-v3-turbo",
    "lang": "auto",
    "chat_model": "chocolatine"
}

full_transcription =  []

# Charger les mod√®les
# diarization_model = Pipeline.from_pretrained("pyannote/speaker-diarization")
def load_core_models():
    """Charge uniquement les mod√®les de base : Whisper + Diarisation (pas Chocolatine)"""
    global Transcriber_Whisper, Transcriber_Whisper_live, last_activity_timestamp, diarization_model
    import time
    
    # Compter seulement les mod√®les core (pas Chocolatine)
    core_models_to_load = sum([1 for model in [Transcriber_Whisper, Transcriber_Whisper_live] if model is None])
    current_model = 0
    
    if core_models_to_load > 0:
        logging.info(f"üé§ Initialisation des mod√®les de base : {core_models_to_load} mod√®le(s) Whisper...")
        logging.info("‚è±Ô∏è Temps estim√©: 2-5 minutes (mod√®les STT)")
    
    # V√©rifier que diarisation est charg√© (normalement charg√© au d√©marrage)
    if diarization_model is None:
        logging.error("‚ùå Mod√®le de diarisation non disponible")
        raise RuntimeError("Le mod√®le de diarisation doit √™tre charg√© au d√©marrage")

    if Transcriber_Whisper is None:
        current_model += 1
        logging.info(f"üì¶ [{current_model}/{core_models_to_load}] D√©marrage du chargement de Whisper Principal...")
        logging.info(f"üîÑ Mod√®le: {model_settings} - Transcription de haute qualit√©")
        logging.info("üíæ Taille estim√©e: ~1-3 GB - Temps estim√©: 1-3 minutes")
        
        start_time = time.time()
        try:
            Transcriber_Whisper = pipeline(
                    "automatic-speech-recognition",
                    model = model_settings,
                    chunk_length_s=30,
                    device=device
                )
            load_time = time.time() - start_time
            logging.info(f"‚úÖ [{current_model}/{core_models_to_load}] Whisper Principal charg√© en {load_time:.1f}s")
            logging.info(f"üéØ GPU utilis√©: {device}")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement de Whisper Principal: {str(e)}")
            raise

    if Transcriber_Whisper_live is None:
        current_model += 1
        logging.info(f"üì¶ [{current_model}/{core_models_to_load}] D√©marrage du chargement de Whisper Live...")
        logging.info("üîÑ Mod√®le: openai/whisper-medium - Transcription en temps r√©el")
        logging.info("üíæ Taille estim√©e: ~1.5 GB - Temps estim√©: 1-2 minutes")
        
        start_time = time.time()
        try:
            Transcriber_Whisper_live = pipeline(
                    "automatic-speech-recognition",
                    model = "openai/whisper-medium",
                    chunk_length_s=30,
                    device=device
                )
            load_time = time.time() - start_time
            logging.info(f"‚úÖ [{current_model}/{core_models_to_load}] Whisper Live charg√© en {load_time:.1f}s")
            logging.info(f"üéØ GPU utilis√©: {device}")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement de Whisper Live: {str(e)}")
            raise

    # R√©sum√© final du chargement des mod√®les core
    if core_models_to_load > 0:
        logging.info("üéâ MOD√àLES DE BASE CHARG√âS AVEC SUCC√àS!")
        logging.info("üìä R√©sum√© des mod√®les STT disponibles:")
        if Transcriber_Whisper: logging.info(f"  ‚úÖ Whisper Principal ({model_settings}): Transcription haute qualit√©")
        if Transcriber_Whisper_live: logging.info("  ‚úÖ Whisper Live (medium): Transcription temps r√©el")
        if diarization_model: logging.info("  ‚úÖ Diarisation: S√©paration des locuteurs")
        logging.info(f"üöÄ Syst√®me STT pr√™t sur {device}!")

    # Optimisation GPU pour les mod√®les Whisper
    if device.type == "cuda":
        logging.info("‚ö° Optimisation GPU: Conversion des mod√®les Whisper en FP16...")
        if Transcriber_Whisper:
            model_post = Transcriber_Whisper.model
            model_post = model_post.half()
            Transcriber_Whisper.model = model_post
            logging.info("  ‚úÖ Whisper Principal optimis√© (FP16)")

        if Transcriber_Whisper_live:
            model_live = Transcriber_Whisper_live.model
            model_live = model_live.half()
            Transcriber_Whisper_live.model = model_live
            logging.info("  ‚úÖ Whisper Live optimis√© (FP16)")
        
        logging.info("‚ö° Optimisation GPU termin√©e - Vitesse accrue!")

    # Mettre √† jour le timestamp d'activit√©
    last_activity_timestamp = time.time()

def load_ai_model():
    """Charge uniquement le mod√®le IA : Chocolatine"""
    global Chocolatine, last_activity_timestamp
    import time
    
    if Chocolatine is None:
        logging.info("ü§ñ D√©marrage du chargement de Chocolatine pour l'IA...")
        logging.info("üîÑ Chocolatine (14B param√®tres) - T√©l√©chargement depuis Hugging Face...")
        logging.info("üíæ Taille estim√©e: ~8-12 GB - Temps estim√©: 3-8 minutes")
        
        start_time = time.time()
        try:
            Chocolatine = pipeline(
                "text-generation", 
                model="jpacifico/Chocolatine-2-14B-Instruct-v2.0",
                device=device
            )
            load_time = time.time() - start_time
            logging.info(f"‚úÖ Chocolatine charg√© en {load_time:.1f}s")
            logging.info(f"üéØ GPU utilis√©: {device}")
            logging.info("ü§ñ IA pr√™te pour l'analyse de transcriptions!")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du chargement de Chocolatine: {str(e)}")
            raise
    else:
        logging.info("‚úÖ Chocolatine d√©j√† charg√©")
        
    # Mettre √† jour le timestamp d'activit√©
    last_activity_timestamp = time.time()

def load_pipeline_diarization(model):
    pipeline_diarization = Pipeline.from_pretrained(
        model,
        cache_dir= HF_cache,
        use_auth_token = HUGGING_FACE_KEY,
    )

    if torch.cuda.is_available():
        pipeline_diarization.to(torch.device("cuda"))

    logging.info(f"Pipeline Diarization d√©plac√©e sur {device}")

    return pipeline_diarization

diarization_model = load_pipeline_diarization("pyannote/speaker-diarization-3.1")
# Charger le mod√®le Chocolatine


model_selected  = [
        "openai/whisper-large-v3-turbo", #Hot
        "openai/whisper-large-v3",
        "openai/whisper-tiny",
        "openai/whisper-small",
        "openai/whisper-medium",
        "openai/whisper-base",
        "openai/whisper-large",
    ]

# whisper_task_transcribe = "transcribe" 
# generate_kwargs={"language": "french"}), 
# generate_kwargs={"task": "translate"}),
# generate_kwargs={"language": "french", "task": "translate"})

model_settings = current_settings.get("model", "openai/whisper-large-v3-turbo")  # Valeur par d√©faut si non d√©finie

# Initialisation des mod√®les √† None
Transcriber_Whisper = None
Transcriber_Whisper_live = None
last_activity_timestamp = None
Chocolatine = None
timeout_seconds = 600  # Timeout en secondes de 10 minutes d'inactivit√©

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Code de d√©marrage
    def monitor_inactivity():
        global last_activity_timestamp
        while True:
            if last_activity_timestamp and (time.time() - last_activity_timestamp > timeout_seconds):
                print("Inactivit√© d√©tect√©e. D√©chargement des mod√®les...")
                unload_models()
                last_activity_timestamp = None
            time.sleep(10)  # V√©rifie toutes les 10 secondes

    # D√©marrer le thread de surveillance d'inactivit√©
    threading.Thread(target=monitor_inactivity, daemon=True).start()

    yield

    # Code d'arr√™t
    unload_models()

# Cr√©er l'application FastAPI avec le gestionnaire de contexte
app = FastAPI(
    title="TranscriMate API",
    description="""
    üéµ **API compl√®te de transcription audio/vid√©o avec IA**
    
    TranscriMate offre **3 modes de transcription** adapt√©s √† diff√©rents besoins :
    
    ## üöÄ **3 Modes Disponibles**
    
    ### **1Ô∏è‚É£ Mode Simple API** (`/transcribe_simple/`)
    * **Usage :** Int√©grations tierces, d√©veloppeurs, applications externes
    * **Fonctionnalit√©s :** Diarisation + Transcription compl√®te
    * **Interface :** Swagger uniquement (pas d'UI frontend)
    * **Retour :** JSON structur√© avec m√©tadonn√©es compl√®tes
    
    ### **2Ô∏è‚É£ Mode Streaming** (`/transcribe_streaming/`)  
    * **Usage :** Interface utilisateur avec feedback temps r√©el
    * **Fonctionnalit√©s :** Diarisation + Transcription avec affichage progressif
    * **Interface :** Frontend Vue.js avec Server-Sent Events
    * **Retour :** Segments progressifs par locuteur
    
    ### **3Ô∏è‚É£ Mode Live** (`/live_transcription/`)
    * **Usage :** Transcription microphone en temps r√©el
    * **Fonctionnalit√©s :** Transcription continue (sans diarisation)
    * **Interface :** WebSocket bidirectionnel
    * **Retour :** Flux audio ‚Üí texte instantan√©
    
    ## üõ†Ô∏è **Technologies**
    
    * **üéØ Diarisation** - pyannote.audio pour s√©paration des locuteurs
    * **üìù Transcription** - Whisper (OpenAI) haute qualit√© & temps r√©el
    * **ü§ñ IA** - Chocolatine/GPT pour analyse des transcriptions
    * **‚ö° GPU** - PyTorch + CUDA pour performances optimales
    * **üåê Cross-platform** - Support Windows/Linux
    """,
    version="1.0.0",
    contact={
        "name": "TranscriMate Support",
        "email": "medhi@transcrimate.ai",
    },
    license_info={
        "name": "MIT License",
    },
    lifespan=lifespan
)

def load_models():
    """Charge TOUS les mod√®les : Whisper + Diarisation + Chocolatine"""
    global Transcriber_Whisper, Transcriber_Whisper_live, last_activity_timestamp, Chocolatine
    import time
    
    total_models = sum([1 for model in [Chocolatine, Transcriber_Whisper, Transcriber_Whisper_live] if model is None])
    
    if total_models > 0:
        logging.info(f"üöÄ Initialisation COMPL√àTE de {total_models} mod√®le(s)...")
        logging.info("‚è±Ô∏è Temps estim√© total: 5-15 minutes (premier chargement)")

    # Charger les mod√®les de base (STT)
    logging.info("üé§ === CHARGEMENT DES MOD√àLES STT ===")
    load_core_models()
    
    # Charger le mod√®le IA
    logging.info("ü§ñ === CHARGEMENT DU MOD√àLE IA ===")
    load_ai_model()

    # R√©sum√© final complet
    logging.info("üéâ === TOUS LES MOD√àLES CHARG√âS AVEC SUCC√àS ===")
    logging.info("üìä R√©sum√© complet des mod√®les disponibles:")
    if Chocolatine: logging.info("  ‚úÖ Chocolatine: G√©n√©ration de texte IA")
    if Transcriber_Whisper: logging.info(f"  ‚úÖ Whisper Principal ({model_settings}): Transcription haute qualit√©")
    if Transcriber_Whisper_live: logging.info("  ‚úÖ Whisper Live (medium): Transcription temps r√©el")
    if diarization_model: logging.info("  ‚úÖ Diarisation: S√©paration des locuteurs")
    logging.info(f"üöÄ Syst√®me COMPLET pr√™t sur {device} - Toutes les fonctionnalit√©s disponibles!")

    # Mettre √† jour le timestamp d'activit√©
    last_activity_timestamp = time.time()



    # Si un GPU est disponible, convertir le mod√®le Whisper en FP16
    if device.type == "cuda":
        logging.info("‚ö° Optimisation GPU: Conversion des mod√®les Whisper en FP16...")
        if Transcriber_Whisper:
            model_post = Transcriber_Whisper.model
            model_post = model_post.half()  # Convertir en FP16
            Transcriber_Whisper.model = model_post  # R√©assigner le mod√®le √† la pipeline
            logging.info("  ‚úÖ Whisper Principal optimis√© (FP16)")

        if Transcriber_Whisper_live:
            model_live = Transcriber_Whisper_live.model
            model_live = model_live.half()  # Convertir en FP16
            Transcriber_Whisper_live.model = model_live  # R√©assigner le mod√®le √† la pipeline
            logging.info("  ‚úÖ Whisper Live optimis√© (FP16)")
        
        logging.info("‚ö° Optimisation GPU termin√©e - Vitesse accrue!")

    # Mettre √† jour le timestamp d'activit√©
    last_activity_timestamp = time.time()

# V√©rifie la configuration GPU compl√®te
@app.get(
    "/device_type/", 
    tags=["üîß Syst√®me"],
    summary="Configuration GPU d√©taill√©e",
    description="Retourne des informations compl√®tes sur la configuration GPU/CPU et les recommandations d'optimisation"
)
async def device_type():
    """
    Endpoint am√©lior√© pour v√©rifier la configuration GPU compl√®te
    Retourne des informations d√©taill√©es sur le GPU et PyTorch
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    gpu_info = {
        "device": str(device),
        "cuda_available": torch.cuda.is_available(),
        "pytorch_version": torch.__version__,
        "cuda_version": None,
        "gpu_count": 0,
        "current_gpu": None,
        "gpu_details": [],
        "memory_info": {},
        "recommendations": []
    }
    
    if torch.cuda.is_available():
        gpu_info["cuda_version"] = torch.version.cuda
        gpu_info["gpu_count"] = torch.cuda.device_count()
        gpu_info["current_gpu"] = torch.cuda.current_device()
        
        # Informations d√©taill√©es pour chaque GPU
        for i in range(torch.cuda.device_count()):
            gpu_props = torch.cuda.get_device_properties(i)
            gpu_detail = {
                "id": i,
                "name": gpu_props.name,
                "memory_total_gb": round(gpu_props.total_memory / (1024**3), 2),
                "compute_capability": f"{gpu_props.major}.{gpu_props.minor}",
                "multi_processor_count": gpu_props.multi_processor_count
            }
            gpu_info["gpu_details"].append(gpu_detail)
        
        # Informations m√©moire GPU actuelle
        current_gpu = torch.cuda.current_device()
        allocated_mb = torch.cuda.memory_allocated(current_gpu) / (1024**2)
        reserved_mb = torch.cuda.memory_reserved(current_gpu) / (1024**2)
        total_mb = torch.cuda.get_device_properties(current_gpu).total_memory / (1024**2)
        
        gpu_info["memory_info"] = {
            "allocated_mb": round(allocated_mb, 2),
            "reserved_mb": round(reserved_mb, 2),
            "total_mb": round(total_mb, 2),
            "usage_percent": round((allocated_mb / total_mb) * 100, 2)
        }
        
        # Recommandations bas√©es sur la configuration
        main_gpu = gpu_info["gpu_details"][0]
        if main_gpu["memory_total_gb"] >= 8:
            gpu_info["recommendations"] = [
                "GPU puissant d√©tect√© (‚â•8GB) - Mod√®les Whisper large recommand√©s",
                "Traitement de longs fichiers audio possible",
                "Diarisation en temps r√©el optimale"
            ]
        elif main_gpu["memory_total_gb"] >= 4:
            gpu_info["recommendations"] = [
                "GPU moyen d√©tect√© (4-8GB) - Mod√®les Whisper medium/base recommand√©s",
                "Attention aux tr√®s longs fichiers audio"
            ]
        else:
            gpu_info["recommendations"] = [
                "GPU l√©ger d√©tect√© (<4GB) - Mod√®les Whisper small/tiny recommand√©s",
                "Traitement par segments recommand√©"
            ]
            
        # V√©rification si PyTorch utilise bien CUDA
        if "+cu" in torch.__version__:
            gpu_info["pytorch_cuda_support"] = "‚úÖ PyTorch CUDA activ√©"
        else:
            gpu_info["pytorch_cuda_support"] = "‚ö†Ô∏è PyTorch CPU uniquement - installer version CUDA"
            
    else:
        gpu_info["recommendations"] = [
            "Aucun GPU CUDA d√©tect√©",
            "Mod√®les Whisper tiny/base pour de bonnes performances CPU",
            "Traitement par petits segments recommand√©"
        ]
        gpu_info["pytorch_cuda_support"] = "‚ùå CUDA non disponible"
    
    return gpu_info


@app.get(
    "/gpu_test/", 
    tags=["üîß Syst√®me"],
    summary="Test de performance GPU",
    description="Teste les performances GPU et v√©rifie que les mod√®les IA utilisent bien le GPU"
)
async def gpu_test():
    """
    Endpoint pour tester les performances GPU avec les mod√®les TranscriMate
    """
    if not torch.cuda.is_available():
        return {
            "status": "error", 
            "message": "GPU non disponible",
            "cuda_available": False
        }
    
    try:
        # Test basique GPU
        device = torch.device("cuda")
        
        # Test de multiplication matricielle
        start_time = time.time()
        a = torch.randn(1000, 1000, device=device)
        b = torch.randn(1000, 1000, device=device)
        c = torch.mm(a, b)
        torch.cuda.synchronize()
        gpu_test_time = time.time() - start_time
        
        # Informations m√©moire apr√®s test
        allocated_mb = torch.cuda.memory_allocated() / (1024**2)
        reserved_mb = torch.cuda.memory_reserved() / (1024**2)
        
        # Test des mod√®les si charg√©s
        models_status = {}
        
        if 'diarization_model' in globals() and diarization_model is not None:
            try:
                # V√©rifier sur quel device est le mod√®le de diarisation
                # Le mod√®le pyannote utilise automatiquement CUDA s'il est disponible
                models_status["diarization"] = {
                    "loaded": True,
                    "device": "cuda" if torch.cuda.is_available() else "cpu",
                    "status": "‚úÖ Mod√®le pyannote sur GPU"
                }
            except Exception as e:
                models_status["diarization"] = {
                    "loaded": True,
                    "error": str(e)
                }
        else:
            models_status["diarization"] = {"loaded": False}
            
        if Transcriber_Whisper is not None:
            try:
                # V√©rifier le device du mod√®le Whisper
                model_device = str(Transcriber_Whisper.device)
                models_status["whisper"] = {
                    "loaded": True,
                    "device": model_device,
                    "model": model_settings,
                    "status": f"‚úÖ Whisper sur {model_device}"
                }
            except Exception as e:
                models_status["whisper"] = {
                    "loaded": True,
                    "error": str(e)
                }
        else:
            models_status["whisper"] = {"loaded": False}
            
        # Nettoyer la m√©moire de test
        del a, b, c
        torch.cuda.empty_cache()
        
        return {
            "status": "success",
            "cuda_available": True,
            "gpu_test_time_ms": round(gpu_test_time * 1000, 2),
            "memory_after_test": {
                "allocated_mb": round(allocated_mb, 2),
                "reserved_mb": round(reserved_mb, 2)
            },
            "models_status": models_status,
            "gpu_name": torch.cuda.get_device_name(),
            "recommendations": [
                "GPU fonctionnel pour les calculs PyTorch",
                "Chargez vos mod√®les avec /initialize/ pour les tester sur GPU",
                "Utilisez /device_type/ pour plus de d√©tails"
            ]
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erreur lors du test GPU: {str(e)}",
            "cuda_available": torch.cuda.is_available()
        }


# Charger les mod√®les √† la demande via la route /initialize/
@app.get(
    "/initialize/", 
    tags=["üîß Syst√®me"],
    summary="Chargement des mod√®les IA",
    description="Charge tous les mod√®les IA (Whisper, pyannote, Chocolatine) en m√©moire GPU/CPU"
)
async def initialize_models():
    import time
    
    logging.info("üöÄ === INITIALISATION DES MOD√àLES DEMAND√âE ===")
    logging.info(f"üïê Heure de d√©but: {time.strftime('%H:%M:%S')}")
    logging.info(f"üíª Appareil utilis√©: {device}")
    
    # V√©rifier l'√©tat actuel des mod√®les
    models_already_loaded = sum([1 for model in [Chocolatine, Transcriber_Whisper, Transcriber_Whisper_live] if model is not None])
    models_to_load = sum([1 for model in [Chocolatine, Transcriber_Whisper, Transcriber_Whisper_live] if model is None])
    
    if models_to_load == 0:
        logging.info("‚úÖ Tous les mod√®les sont d√©j√† charg√©s!")
        return {"message": "Tous les mod√®les sont d√©j√† charg√©s", "status": "already_loaded"}
    
    logging.info(f"üìä √âtat: {models_already_loaded} mod√®le(s) d√©j√† charg√©(s), {models_to_load} √† charger")
    
    start_total = time.time()
    try:
        load_models()
        total_time = time.time() - start_total
        
        logging.info("üéâ === INITIALISATION TERMIN√âE ===")
        logging.info(f"‚è±Ô∏è Temps total: {total_time:.1f} secondes")
        logging.info(f"üïê Heure de fin: {time.strftime('%H:%M:%S')}")
        
        return {
            "message": "Mod√®les charg√©s avec succ√®s", 
            "status": "loaded",
            "total_time_seconds": round(total_time, 1),
            "device": str(device)
        }
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de l'initialisation: {str(e)}")
        return {"message": f"Erreur lors du chargement: {str(e)}", "status": "error"}

@app.get(
    "/model-status/", 
    tags=["üîß Syst√®me"],
    summary="√âtat des mod√®les",
    description="Affiche l'√©tat actuel de tous les mod√®les IA et leur statut de chargement"
)
async def get_models_status():
    models_status = {
        "device": str(device),
        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
        "models": {
            "chocolatine": {
                "loaded": Chocolatine is not None,
                "type": "text-generation",
                "model_name": "jpacifico/Chocolatine-2-14B-Instruct-v2.0" if Chocolatine else None
            },
            "whisper_principal": {
                "loaded": Transcriber_Whisper is not None,
                "type": "speech-recognition",
                "model_name": model_settings if Transcriber_Whisper else None
            },
            "whisper_live": {
                "loaded": Transcriber_Whisper_live is not None,
                "type": "speech-recognition-live", 
                "model_name": "openai/whisper-medium" if Transcriber_Whisper_live else None
            },
            "diarization": {
                "loaded": diarization_model is not None,
                "type": "speaker-diarization",
                "model_name": "pyannote/speaker-diarization-3.1" if diarization_model else None
            }
        }
    }
    
    total_loaded = sum([1 for model in models_status["models"].values() if model["loaded"]])
    models_status["summary"] = {
        "total_models": 4,
        "loaded_models": total_loaded,
        "all_loaded": total_loaded == 4,
        "percentage": round((total_loaded / 4) * 100, 1)
    }
    
    logging.info(f"üìä √âtat des mod√®les demand√© - {total_loaded}/4 charg√©s ({models_status['summary']['percentage']}%)")
    
    return models_status

@app.get(
    "/keep_alive/", 
    tags=["üîß Syst√®me"],
    summary="Maintien de session",
    description="Met √† jour le timestamp d'activit√© pour √©viter le d√©chargement automatique des mod√®les"
)
async def keep_alive():
    global last_activity_timestamp, Transcriber_Whisper, Transcriber_Whisper_live

    # Charger le mod√®le uniquement s'il n'est pas d√©j√† charg√©
    if Transcriber_Whisper is None or Transcriber_Whisper_live is None:
        load_models()

    # Mettre √† jour le timestamp d'activit√© chaque fois que le frontend fait un ping
    last_activity_timestamp = time.time()
    return {"message": "Timestamp d'activit√© mis √† jour"}


# D√©charger les mod√®les pour √©conomiser la m√©moire
def unload_models():
    global Transcriber_Whisper, Transcriber_Whisper_live
    if Transcriber_Whisper:
        del Transcriber_Whisper
        Transcriber_Whisper = None
    if Transcriber_Whisper_live:
        del Transcriber_Whisper_live
        Transcriber_Whisper_live = None
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("Mod√®les d√©charg√©s pour √©conomiser la m√©moire.")

# Surveiller l'inactivit√© pour d√©charger les mod√®les si n√©cessaire
def monitor_inactivity():
    global last_activity_timestamp
    while True:
        if last_activity_timestamp and (time.time() - last_activity_timestamp > timeout_seconds):
            print("Inactivit√© d√©tect√©e. D√©chargement des mod√®les...")
            unload_models()
            last_activity_timestamp = None
        time.sleep(10)  # V√©rifie toutes les 10 secondes


generate_kwargs_live = {
    "max_new_tokens": 224,  # Limiter la taille pour acc√©l√©rer les pr√©dictions en streaming.
    "num_beams": 1,  # D√©codage rapide (greedy decoding).
    "condition_on_prev_tokens": False,  # D√©sactiver le contexte entre les segments pour r√©duire la latence.
    "compression_ratio_threshold": 1.35,  # Standard pour filtrer les segments improbables.
    "temperature": 0.0,  # Pr√©f√©rer des transcriptions conservatrices en temps r√©el.
    "logprob_threshold": -1.0,  # Filtrer les tokens peu probables.
    "no_speech_threshold": 0.6,  # Garder une tol√©rance moyenne pour les silences.
}

generate_kwargs_aposteriori = {
    "max_new_tokens": 336,  # Autoriser des transcriptions plus longues (max 448)
    "num_beams": 4,  # Beam search pour am√©liorer la qualit√© de la transcription.
    "condition_on_prev_tokens": True,  # Maintenir le contexte entre les segments.
    "compression_ratio_threshold": 2.4,  # Tol√©rer des segments compress√©s.
    "temperature": 0.4,  # √âquilibre entre diversit√© et pr√©cision.
    "logprob_threshold": -1.5,  # Filtrer les tokens improbables.
    "no_speech_threshold": 0.4,  # Accepter plus de segments contenant des silences.
}


app.add_middleware(
    CORSMiddleware,
    # allow_origins=["http://localhost:8080/"],  # URL de Vue.js
    # allow_origins=[f"{server_url}:8080"],  # URL de Vue.js
    allow_origins=["*"],  # URL de Vue.js
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post(
    "/diarization/", 
    tags=["üéµ Audio"],
    summary="Diarisation (s√©paration des locuteurs)",
    description="Analyse un fichier audio/vid√©o pour identifier et s√©parer les diff√©rents locuteurs sans transcription"
)
async def upload_file(file: UploadFile = File(...)):
    # V√©rifier et initialiser les mod√®les si n√©cessaire
    global diarization_model
    if diarization_model is None:
        logging.info("Initialisation des mod√®les pour diarization...")
        await load_models()
        
        # V√©rifier que l'initialisation a r√©ussi
        if diarization_model is None:
            logging.error("√âchec de l'initialisation du mod√®le de diarisation")
            raise HTTPException(status_code=500, detail="Impossible d'initialiser le mod√®le de diarisation. V√©rifiez les logs serveur.")
    
    async with async_temp_manager_context("diarization") as temp_manager:
        # Lire le fichier upload√©
        file_data = await file.read()
        file_path = temp_manager.create_temp_file(file.filename, file_data)

        full_transcription_text = "\n"

        # D√©tection de l'extension du fichier (s√©curis√©e)
        file_extension = os.path.splitext(file.filename)[1].lower()
        logging.info(f"Extension d√©tect√©e {file_extension}.")
        logging.info(f"Fichier {file.filename} sauvegard√© avec succ√®s.")
        
        # Si le fichier est un fichier audio (formats courants)
        if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
            logging.info(f"fichier audio d√©tect√©: {file_extension}.")
            # Charger le fichier audio avec Pydub
            audio = AudioSegment.from_file(file_path)

        elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
            logging.info(f"fichier vid√©o d√©tect√©: {file_extension}.")
            VideoFileClip(file_path)
            audio = AudioSegment.from_file(file_path, format=file.filename)

        logging.info(f"Conversion du {file.filename} en mono 16kHz.")

        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(16000)
        
        # Cr√©er un chemin pour le fichier audio converti
        audio_path = temp_manager.get_temp_path_with_suffix(".wav")
        
        logging.info(f"Sauvegarde de la piste audio dans {audio_path}.")

        audio.export(audio_path, format="wav")

        # V√©rification si le fichier existe
        if not os.path.exists(audio_path):
            logging.error(f"Le fichier {audio_path} n'existe pas.")
            raise HTTPException(status_code=404, detail=f"Le fichier {audio_path} n'existe pas.")

        with ProgressHook() as hook:
            logging.debug(f"Diarization d√©marr√©e")
            diarization = diarization_model(audio_path, hook=hook)
            logging.debug(f"Diarization termin√©e {diarization}")

        diarization_json = convert_tracks_to_json(diarization)
        logging.debug(f"R√©sultat de la diarization {diarization_json}")
        return diarization_json


class Settings(BaseModel):
    task: StrictStr = "transcribe"
    model: StrictStr = "openai/whisper-large-v3-turbo"  # Valeur par d√©faut
    lang: StrictStr = "auto"  # Valeur par d√©faut
    chat_model: StrictStr = "chocolatine"

    def __init__(self, **data):
        super().__init__(**data)
        self.validate()

    def validate(self):
        if self.task not in ["transcribe", "translate"]:
            raise ValueError("Task must be 'transcribe' or 'translate'")

        allowed_models = [
            "openai/whisper-large-v3-turbo",
            "openai/whisper-large-v3",
            "openai/whisper-tiny",
            "openai/whisper-small",
            "openai/whisper-medium",
            "openai/whisper-base",
            "openai/whisper-large"
        ]
        if self.model not in allowed_models:
            raise ValueError("Invalid model")

        if self.lang not in ["auto", "fr", "en"]:
            raise ValueError("Language must be 'fr', 'en', or 'auto'")


@app.post(
    "/settings/", 
    tags=["‚öôÔ∏è Configuration"],
    summary="Mise √† jour des param√®tres",
    description="Configure les param√®tres de transcription : mod√®le Whisper, langue, t√¢che (transcription/traduction)"
)
def update_settings(settings: Settings):
    global current_settings
    # Logique de mise √† jour des param√®tres c√¥t√© backend
    # Enregistrez les param√®tres dans une base de donn√©es ou un fichier de configuration, par exemple
    current_settings = settings.model_dump()  # Mettez √† jour la variable globale
    logging.info(f"Settings: {current_settings}, task: {current_settings['task']}")

    return {"message": "Param√®tres mis √† jour avec succ√®s"}


async def process_streaming_audio(file_path: str, file_extension: str, filename: str):
    """G√©n√©rateur async pour le traitement streaming avec Server-Sent Events"""
    logging.info("üöÄ === D√âBUT DU STREAMING process_streaming_audio() ===")
    start_total = time.time()
    
    # Envoyer le statut de d√©but d'extraction
    extraction_status = json.dumps({'extraction_audio_status': 'extraction_audio_ongoing', 'message': 'Extraction audio en cours ...'})
    yield f"{extraction_status}\n"
    logging.info(f"üì§ Message envoy√© au frontend: {extraction_status}")

    # Faire l'extraction audio dans le streaming pour un feedback temps r√©el
    audio_path = None
    try:
        start_extraction = time.time()
        
        # Si le fichier est un fichier audio (formats courants)
        if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
            logging.info(f"üéµ Fichier audio d√©tect√©: {file_extension}")

            logging.debug("üîÑ Chargement du fichier audio avec pydub...")
            start_load = time.time()
            audio = AudioSegment.from_file(file_path)
            end_load = time.time()
            logging.info(f"üéµ Audio charg√© en {end_load - start_load:.2f}s - Dur√©e: {len(audio)/1000:.2f}s, Channels: {audio.channels}, Sample Rate: {audio.frame_rate}Hz")
            
            logging.debug(f"üîÑ Conversion du {filename} en mono 16kHz...")
            start_convert = time.time()
            audio = audio.set_channels(1)
            audio = audio.set_frame_rate(16000)
            end_convert = time.time()
            logging.info(f"üîÑ Conversion termin√©e en {end_convert - start_convert:.2f}s")
            
            # Cr√©er un chemin pour le fichier audio converti
            import tempfile
            audio_path = tempfile.mktemp(suffix=".wav")
            logging.info(f"üíæ Sauvegarde de la piste audio dans {audio_path}")
            start_export = time.time()
            audio.export(audio_path, format="wav")
            end_export = time.time()
            logging.info(f"üíæ Exportation termin√©e en {end_export - start_export:.2f}s")

        elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
            logging.info(f"üé¨ Fichier vid√©o d√©tect√©: {file_extension}")
            
            logging.debug("üîÑ Chargement de la vid√©o avec MoviePy...")
            start_video_load = time.time()
            video_clip = VideoFileClip(file_path)
            end_video_load = time.time()
            logging.info(f"üé¨ Vid√©o charg√©e en {end_video_load - start_video_load:.2f}s - Dur√©e: {video_clip.duration:.2f}s")
            
            logging.debug("üîÑ Extraction audio de la vid√©o...")
            start_audio_extract = time.time()
            
            # Utiliser le type d√©tect√© ou l'extension du fichier
            file_type = filetype.guess(file_path)
            format_to_use = file_type.extension if file_type else file_extension[1:]  # Enlever le point
            
            audio = AudioSegment.from_file(file_path, format=format_to_use)
            end_audio_extract = time.time()
            logging.info(f"üéµ Audio extrait en {end_audio_extract - start_audio_extract:.2f}s - Dur√©e: {len(audio)/1000:.2f}s")

            logging.debug(f"üîÑ Conversion du {filename} en mono 16kHz...")
            start_convert_video = time.time()
            audio = audio.set_channels(1)
            audio = audio.set_frame_rate(16000)
            end_convert_video = time.time()
            logging.info(f"üîÑ Conversion vid√©o termin√©e en {end_convert_video - start_convert_video:.2f}s")
            
            # Cr√©er un chemin pour le fichier audio converti (utiliser tempfile pour √©viter le race condition)
            import tempfile
            audio_path = tempfile.mktemp(suffix=".wav")
            logging.info(f"üíæ Sauvegarde de la piste audio dans {audio_path}")
            start_export_video = time.time()
            audio.export(audio_path, format="wav")
            end_export_video = time.time()
            logging.info(f"üíæ Exportation vid√©o termin√©e en {end_export_video - start_export_video:.2f}s")
            
            # Lib√©rer la m√©moire
            video_clip.close()
            logging.debug("üóëÔ∏è Ressources vid√©o lib√©r√©es")
            
        else:
            # Format de fichier non support√©
            logging.error(f"‚ùå Format de fichier non support√©: {file_extension}")
            logging.info("üìã Formats support√©s: .mp3, .wav, .aac, .ogg, .flac, .m4a, .mp4, .mov, .3gp, .mkv")
            error_msg = json.dumps({'error': 'unsupported_format', 'message': f'Format de fichier non support√©: {file_extension}'})
            yield f"{error_msg}\n"
            logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
            return

        end_extraction = time.time()
        total_extraction_time = end_extraction - start_extraction
        logging.info(f"‚è±Ô∏è Extraction audio totale termin√©e en {total_extraction_time:.2f}s")

        # V√©rification si le fichier existe
        if not os.path.exists(audio_path):
            logging.error(f"‚ùå Le fichier audio converti n'existe pas: {audio_path}")
            error_msg = json.dumps({'error': 'file_not_found', 'message': f'Le fichier {audio_path} n\'existe pas.'})
            yield f"{error_msg}\n"
            logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
            return
        
        # V√©rification suppl√©mentaire de l'existence du fichier original
        if not os.path.exists(file_path):
            logging.error(f"‚ùå Le fichier original n'existe plus: {file_path}")
            error_msg = json.dumps({'error': 'original_file_not_found', 'message': f'Le fichier original {file_path} n\'existe plus.'})
            yield f"{error_msg}\n"
            logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
            return

        # V√©rifier la taille du fichier cr√©√©
        file_size = os.path.getsize(audio_path)
        logging.info(f"‚úÖ Fichier audio converti cr√©√© avec succ√®s - Taille: {file_size} bytes")

        # Envoyer le statut de fin d'extraction
        extraction_done = json.dumps({'extraction_audio_status': 'extraction_audio_done', 'message': 'Extraction audio termin√©e!'})
        yield f"{extraction_done}\n"
        logging.info(f"üì§ ‚úÖ Message de fin d'extraction envoy√©: {extraction_done}")

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logging.error(f"‚ùå ERREUR lors de l'extraction audio:")
        logging.error(f"‚ùå Type d'erreur: {type(e).__name__}")
        logging.error(f"‚ùå Message: {str(e)}")
        logging.error(f"‚ùå Stack trace complet:\n{error_details}")
        
        error_msg = json.dumps({
            'error': 'extraction_failed', 
            'message': f'Erreur lors de l\'extraction audio: {str(e)}',
            'error_type': type(e).__name__
        })
        yield f"{error_msg}\n"
        logging.debug(f"üì§ Message d'erreur d√©taill√© envoy√©: {error_msg}")
        return

    # √âtape 1 : Diarisation
    logging.info(f"üéØ === D√âBUT DE LA DIARISATION ===")
    logging.info(f"üéØ Fichier audio √† traiter: {audio_path}")
    
    # V√©rification finale avant diarisation
    if not os.path.exists(audio_path):
        logging.error(f"‚ùå CRITIQUE: Fichier audio inexistant juste avant diarisation: {audio_path}")
        error_msg = json.dumps({
            'error': 'audio_file_missing_before_diarization', 
            'message': f'Le fichier audio {audio_path} n\'existe plus avant la diarisation.',
            'error_type': 'FileNotFoundError'
        })
        yield f"{error_msg}\n"
        return
        
    logging.info(f"üéØ Taille du fichier: {os.path.getsize(audio_path)} bytes")
    
    start_diarization_total = time.time()

    # Envoi du statut "en cours"
    start_diarization = json.dumps({'status': 'diarization_processing', 'message': 'S√©paration des voix en cours, patience est m√®re de vertu ...'})
    yield f"{start_diarization}\n"
    await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse
    logging.info(start_diarization)

    logging.debug(f"Diarization d√©marr√©e pour le fichier {audio_path}")

    try:
        with ProgressHook() as hook:
            diarization = diarization_model(audio_path, hook=hook)
        # diarization = diarization_model(audio_path)
    except Exception as e:
        logging.error(f"Erreur pendant la diarisation : {str(e)}")

    # Envoi final du statut pour indiquer la fin
    end_diarization = json.dumps({'status': 'diarization_done', 'message': 'S√©paration des voix termin√©e.'})
    yield f"{end_diarization}\n"

    logging.debug(f"Diarization termin√©e {diarization}")

    try:
        diarization_json = convert_tracks_to_json(diarization)
        logging.info(f"Taille des donn√©es de diarisation en JSON : {len(json.dumps(diarization_json))} octets")
    except Exception as e:
        logging.error(f"Erreur pendant la conversion de la diarisation en JSON : {str(e)}")
        yield json.dumps({"status": "error", "message": f"Erreur pendant la conversion en JSON : {str(e)}"}) + "\n"
        return

    logging.debug(f"R√©sultat de la diarization {diarization_json}")

    await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse
    logging.info(end_diarization)

    diarization_json = convert_tracks_to_json(diarization)

    # Envoyer la diarisation compl√®te d'abord
    logging.info(f"{json.dumps({'diarization': diarization_json})}")
    yield f"{json.dumps({'diarization': diarization_json})}\n"
    await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse

    # Exporter les segments pour chaque locuteur
    total_chunks = len(list(diarization.itertracks(yield_label=True))) 
    logging.info(f"total_turns: {total_chunks}")
    
    turn_number = 0
    full_transcription = []
    # for turn, _, speaker in tqdm(diarization.itertracks(yield_label=True), total=total_turns, desc="Processing turns"):
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        turn_number += 1
        logging.info(f"Tour {turn_number}/{total_chunks}")

        # √âtape 2 : Transcription pour chaque segment
        start_ms = int(turn.start * 1000)  # Convertir de secondes en millisecondes
        end_ms = int(turn.end * 1000)

        # Extraire le segment audio correspondant au speaker
        segment_audio = audio[start_ms:end_ms]

        # Sauvegarder le segment temporairement pour Whisper
        segment_path = tempfile.mktemp(suffix=".wav")
        segment_audio.export(segment_path, format="wav")

        logging.info(f"----> Transcription d√©mar√©e avec le model et la task <----")

        task = current_settings.get("task", "transcribe")
        if task != "transcribe":
            generate_kwargs = {"language": "english"} 
        else:
            generate_kwargs = {} 

        # Transcrire ce segment avec Whisper
        transcription = Transcriber_Whisper(
            segment_path,
            return_timestamps=True,
            generate_kwargs=generate_kwargs
        )

        # Supprimer le fichier de segment une fois transcrit
        # os.remove(segment_path)

        segment = {
            "speaker": speaker,
            "text": transcription,
            "start_time": turn.start,
            "end_time": turn.end,
            "audio_url": f"{server_url}/segment_audio/{os.path.basename(segment_path)}"  # URL du fichier audio
        }

        logging.info(f"Transcription du speaker {speaker} du segment de {turn.start} √† {turn.end} termin√©e\n R√©sultat de la transcription {segment}")
        full_transcription.append(segment)

        yield f"{json.dumps(segment)}\n"  # Envoi du segment de transcription en JSON
        await asyncio.sleep(0)  # Forcer l'envoi de chaque chunk

        logging.info(f"Transcription du speaker {speaker} pour le segment de {turn.start} √† {turn.end} termin√©e")

    # Fin du streaming
    logging.info(f"->> fin de transcription <<")
    logging.info(full_transcription)


@app.post(
    "/transcribe_streaming/", 
    tags=["üéµ Audio"],
    summary="üîÑ Interface Progressive - Streaming Temps R√©el",
    description="**Mode 2/3** - Traitement complet avec diarisation + transcription et affichage progressif (Server-Sent Events pour frontend Vue.js)"
)
async def upload_file_streaming(file: UploadFile = File(...)):
    logging.info("=== D√âBUT UPLOAD_FILE_STREAMING ===")
    logging.info(f"üìÅ Fichier re√ßu: {file.filename}, Type: {file.content_type}, Taille: {file.size if hasattr(file, 'size') else 'inconnue'}")
    
    # V√©rifier et initialiser les mod√®les si n√©cessaire
    global Transcriber_Whisper, diarization_model
    logging.debug(f"üîç √âtat des mod√®les - Transcriber_Whisper: {'‚úÖ Charg√©' if Transcriber_Whisper is not None else '‚ùå Non charg√©'}")
    logging.debug(f"üîç √âtat des mod√®les - diarization_model: {'‚úÖ Charg√©' if diarization_model is not None else '‚ùå Non charg√©'}")
    
    if Transcriber_Whisper is None or diarization_model is None:
        logging.warning("‚ö†Ô∏è Mod√®les STT non initialis√©s, d√©marrage du chargement des mod√®les de base...")
        start_init = time.time()
        load_core_models()  # Charger seulement les mod√®les STT (Whisper + diarisation)
        end_init = time.time()
        logging.info(f"‚è±Ô∏è Mod√®les STT initialis√©s en {end_init - start_init:.2f}s")
        
        # V√©rifier que l'initialisation a r√©ussi
        if Transcriber_Whisper is None or diarization_model is None:
            logging.error("‚ùå √âCHEC de l'initialisation des mod√®les")
            raise HTTPException(status_code=500, detail="Impossible d'initialiser les mod√®les. V√©rifiez les logs serveur.")
        else:
            logging.info("‚úÖ Mod√®les initialis√©s avec succ√®s")
    
    logging.info("üìÇ Initialisation du gestionnaire de fichiers temporaires...")
    async with async_temp_manager_context("transcribe_streaming") as temp_manager:
        # Lire le fichier upload√©
        logging.debug("üìñ Lecture du fichier upload√©...")
        start_read = time.time()
        file_data = await file.read()
        end_read = time.time()
        logging.info(f"üìñ Fichier lu en {end_read - start_read:.2f}s - Taille: {len(file_data)} bytes")
        
        # Cr√©er le fichier temporaire
        logging.debug("üíæ Cr√©ation du fichier temporaire...")
        file_path = temp_manager.create_temp_file(file.filename, file_data)
        logging.info(f"üíæ Fichier temporaire cr√©√©: {file_path}")
        
        # V√©rification imm√©diate de l'existence du fichier
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            logging.info(f"‚úÖ Fichier temporaire confirm√© - Taille: {file_size} bytes")
        else:
            logging.error(f"‚ùå ERREUR CRITIQUE: Le fichier temporaire n'a pas √©t√© cr√©√©: {file_path}")
            raise HTTPException(status_code=500, detail="Impossible de cr√©er le fichier temporaire")
        
        # D√©tection de l'extension du fichier (s√©curis√©e)
        file_extension = os.path.splitext(file.filename)[1].lower()
        logging.info(f"üîç Extension d√©tect√©e: {file_extension}")

        # Le nettoyage est automatique avec async_temp_manager_context
        # Retourner la r√©ponse streaming dans le contexte pour √©viter le race condition
        return StreamingResponse(
            process_streaming_audio(file_path, file_extension, file.filename), 
            media_type="application/json"
        )


# Endpoint pour g√©n√©rer les URLs des segments audio
        audio_path = None
        try:
            start_extraction = time.time()
            
            # Si le fichier est un fichier audio (formats courants)
            if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
                logging.info(f"üéµ Fichier audio d√©tect√©: {file_extension}")

                logging.debug("üîÑ Chargement du fichier audio avec pydub...")
                start_load = time.time()
                audio = AudioSegment.from_file(file_path)
                end_load = time.time()
                logging.info(f"üéµ Audio charg√© en {end_load - start_load:.2f}s - Dur√©e: {len(audio)/1000:.2f}s, Channels: {audio.channels}, Sample Rate: {audio.frame_rate}Hz")
                
                logging.debug(f"üîÑ Conversion du {file.filename} en mono 16kHz...")
                start_convert = time.time()
                audio = audio.set_channels(1)
                audio = audio.set_frame_rate(16000)
                end_convert = time.time()
                logging.info(f"üîÑ Conversion termin√©e en {end_convert - start_convert:.2f}s")
                    
                    # Cr√©er un chemin pour le fichier audio converti (utiliser tempfile pour √©viter le race condition)
                import tempfile
                audio_path = tempfile.mktemp(suffix=".wav")
                logging.info(f"üíæ Sauvegarde de la piste audio dans {audio_path}")
                start_export = time.time()
                audio.export(audio_path, format="wav")
                end_export = time.time()
                logging.info(f"üíæ Exportation termin√©e en {end_export - start_export:.2f}s")

            elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
                logging.info(f"üé¨ Fichier vid√©o d√©tect√©: {file_extension}")
                
                logging.debug("üîÑ Chargement de la vid√©o avec MoviePy...")
                start_video_load = time.time()
                video_clip = VideoFileClip(file_path)
                end_video_load = time.time()
                logging.info(f"üé¨ Vid√©o charg√©e en {end_video_load - start_video_load:.2f}s - Dur√©e: {video_clip.duration:.2f}s")
                
                logging.debug("üîÑ Extraction audio de la vid√©o...")
                start_audio_extract = time.time()
                
                # Utiliser le type d√©tect√© ou l'extension du fichier
                format_to_use = file_type.extension if file_type else file_extension[1:]  # Enlever le point
                logging.debug(f"üîÑ Format utilis√© pour l'extraction: {format_to_use}")
                
                audio = AudioSegment.from_file(file_path, format=format_to_use)
                end_audio_extract = time.time()
                logging.info(f"üéµ Audio extrait en {end_audio_extract - start_audio_extract:.2f}s - Dur√©e: {len(audio)/1000:.2f}s")

                logging.debug(f"üîÑ Conversion du {file.filename} en mono 16kHz...")
                start_convert_video = time.time()
                audio = audio.set_channels(1)
                audio = audio.set_frame_rate(16000)
                end_convert_video = time.time()
                logging.info(f"üîÑ Conversion vid√©o termin√©e en {end_convert_video - start_convert_video:.2f}s")
                
                # Cr√©er un chemin pour le fichier audio converti (utiliser tempfile pour √©viter le race condition)
                import tempfile
                audio_path = tempfile.mktemp(suffix=".wav")
                logging.info(f"üíæ Sauvegarde de la piste audio dans {audio_path}")
                start_export_video = time.time()
                audio.export(audio_path, format="wav")
                end_export_video = time.time()
                logging.info(f"üíæ Exportation vid√©o termin√©e en {end_export_video - start_export_video:.2f}s")
                
                # Lib√©rer la m√©moire
                video_clip.close()
                logging.debug("üóëÔ∏è Ressources vid√©o lib√©r√©es")
                
            else:
                # Format de fichier non support√©
                logging.error(f"‚ùå Format de fichier non support√©: {file_extension}")
                logging.info("üìã Formats support√©s: .mp3, .wav, .aac, .ogg, .flac, .m4a, .mp4, .mov, .3gp, .mkv")
                error_msg = json.dumps({'error': 'unsupported_format', 'message': f'Format de fichier non support√©: {file_extension}'})
                yield f"{error_msg}\n"
                logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
                return

            end_extraction = time.time()
            total_extraction_time = end_extraction - start_extraction
            logging.info(f"‚è±Ô∏è Extraction audio totale termin√©e en {total_extraction_time:.2f}s")

            # V√©rification si le fichier existe
            if not os.path.exists(audio_path):
                logging.error(f"‚ùå Le fichier audio converti n'existe pas: {audio_path}")
                error_msg = json.dumps({'error': 'file_not_found', 'message': f'Le fichier {audio_path} n\'existe pas.'})
                yield f"{error_msg}\n"
                logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
                return
                
            # V√©rification suppl√©mentaire de l'existence du fichier original
            if not os.path.exists(file_path):
                logging.error(f"‚ùå Le fichier original n'existe plus: {file_path}")
                error_msg = json.dumps({'error': 'original_file_not_found', 'message': f'Le fichier original {file_path} n\'existe plus.'})
                yield f"{error_msg}\n"
                logging.debug(f"üì§ Message d'erreur envoy√©: {error_msg}")
                return

            # V√©rifier la taille du fichier cr√©√©
            file_size = os.path.getsize(audio_path)
            logging.info(f"‚úÖ Fichier audio converti cr√©√© avec succ√®s - Taille: {file_size} bytes")

            # Envoyer le statut de fin d'extraction
            extraction_done = json.dumps({'extraction_audio_status': 'extraction_audio_done', 'message': 'Extraction audio termin√©e!'})
            yield f"{extraction_done}\n"
            logging.info(f"üì§ ‚úÖ Message de fin d'extraction envoy√©: {extraction_done}")

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logging.error(f"‚ùå ERREUR lors de l'extraction audio:")
            logging.error(f"‚ùå Type d'erreur: {type(e).__name__}")
            logging.error(f"‚ùå Message: {str(e)}")
            logging.error(f"‚ùå Stack trace complet:\n{error_details}")
            
            error_msg = json.dumps({
                'error': 'extraction_failed', 
                'message': f'Erreur lors de l\'extraction audio: {str(e)}',
                'error_type': type(e).__name__
            })
            yield f"{error_msg}\n"
            logging.debug(f"üì§ Message d'erreur d√©taill√© envoy√©: {error_msg}")
            return

        # √âtape 1 : Diarisation
        logging.info(f"üéØ === D√âBUT DE LA DIARISATION ===")
        logging.info(f"üéØ Fichier audio √† traiter: {audio_path}")
        
        # V√©rification finale avant diarisation
        if not os.path.exists(audio_path):
            logging.error(f"‚ùå CRITIQUE: Fichier audio inexistant juste avant diarisation: {audio_path}")
            error_msg = json.dumps({
                'error': 'audio_file_missing_before_diarization', 
                'message': f'Le fichier audio {audio_path} n\'existe plus avant la diarisation.',
                'error_type': 'FileNotFoundError'
            })
            yield f"{error_msg}\n"
            return
            
        logging.info(f"üéØ Taille du fichier: {os.path.getsize(audio_path)} bytes")
        
        start_diarization_total = time.time()

        # Envoi du statut "en cours"
        start_diarization = json.dumps({'status': 'diarization_processing', 'message': 'S√©paration des voix en cours, patience est m√®re de vertu ...'})
        yield f"{start_diarization}\n"
        await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse
        logging.info(start_diarization)

        logging.debug(f"Diarization d√©marr√©e pour le fichier {audio_path}")

        try:
            with ProgressHook() as hook:
                diarization = diarization_model(audio_path, hook=hook)
            # diarization = diarization_model(audio_path)
        except Exception as e:
            logging.error(f"Erreur pendant la diarisation : {str(e)}")

        # Envoi final du statut pour indiquer la fin
        end_diarization = json.dumps({'status': 'diarization_done', 'message': 'S√©paration des voix termin√©e.'})
        yield f"{end_diarization}\n"

        logging.debug(f"Diarization termin√©e {diarization}")

            # diarization_json = convert_tracks_to_json(diarization)

        try:
            diarization_json = convert_tracks_to_json(diarization)
            logging.info(f"Taille des donn√©es de diarisation en JSON : {len(json.dumps(diarization_json))} octets")
        except Exception as e:
            logging.error(f"Erreur pendant la conversion de la diarisation en JSON : {str(e)}")
            yield json.dumps({"status": "error", "message": f"Erreur pendant la conversion en JSON : {str(e)}"}) + "\n"
            return

        logging.debug(f"R√©sultat de la diarization {diarization_json}")

        await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse
        logging.info(end_diarization)

        diarization_json = convert_tracks_to_json(diarization)

        # Envoyer la diarisation compl√®te d'abord
        logging.info(f"{json.dumps({'diarization': diarization_json})}")
        yield f"{json.dumps({'diarization': diarization_json})}\n"
        await asyncio.sleep(0.1)  # Petit d√©lai pour forcer l'envoi de la premi√®re r√©ponse

        # Exporter les segments pour chaque locuteur
        total_chunks = len(list(diarization.itertracks(yield_label=True))) 
        logging.info(f"total_turns: {total_chunks}")
        
        turn_number = 0
        # for turn, _, speaker in tqdm(diarization.itertracks(yield_label=True), total=total_turns, desc="Processing turns"):
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            turn_number += 1
            logging.info(f"Tour {turn_number}/{total_chunks}")

            # √âtape 2 : Transcription pour chaque segment
            start_ms = int(turn.start * 1000)  # Convertir de secondes en millisecondes
            end_ms = int(turn.end * 1000)

            # Extraire le segment audio correspondant au speaker
            segment_audio = audio[start_ms:end_ms]

            # Sauvegarder le segment temporairement pour Whisper
            segment_path = temp_manager.get_temp_path_with_suffix(".wav")
            segment_audio.export(segment_path, format="wav")

            logging.info(f"----> Transcription d√©mar√©e avec le model <{model_settings}> et la task <{task}> <----")

                # generate_kwargs = {
                #     "max_new_tokens": 448,
                #     "num_beams": 1,
                #     "condition_on_prev_tokens": False, # Si activ√©, Whisper prend en compte les tokens pr√©c√©demment g√©n√©r√©s pour conditionner la g√©n√©ration des tokens actuels.
                #     "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)
                #     "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
                #     "logprob_threshold": -1.0,
                #     "no_speech_threshold": 0.6,
                #     "return_timestamps": True,
                # }

            if current_settings['task'] != "transcribe":
                generate_kwargs={
                    "language": "english", 
                } 

            else:
                generate_kwargs={
                } 

            # Transcrire ce segment avec Whisper
            transcription = Transcriber_Whisper(
                segment_path,
                return_timestamps = True,
                generate_kwargs= generate_kwargs |  generate_kwargs_aposteriori
            )

            # Supprimer le fichier de segment une fois transcrit
            # os.remove(segment_path)

            segment = {
                "speaker": speaker,
                "text": transcription,
                "start_time": turn.start,
                "end_time": turn.end,
                "audio_url": f"{server_url}/segment_audio/{os.path.basename(segment_path)}"  # URL du fichier audio
            }

            logging.info(f"Transcription du speaker {speaker} du segment de {turn.start} √† {turn.end} termin√©e\n R√©sultat de la transcription {segment}")
            full_transcription.append(segment)

            yield f"{json.dumps(segment)}\n"  # Envoi du segment de transcription en JSON
            await asyncio.sleep(0)  # Forcer l'envoi de chaque chunk

            logging.info(f"Transcription du speaker {speaker} pour le segment de {turn.start} √† {turn.end} termin√©e")

            # Fin du streaming
            logging.info(f"->> fin de transcription <<")
            logging.info(full_transcription)

        # Le nettoyage est automatique avec async_temp_manager_context
        # Retourner la r√©ponse streaming dans le contexte pour √©viter le race condition
        return StreamingResponse(live_process_audio(), media_type="application/json")


# Endpoint pour g√©n√©rer les URLs des segments audio
@app.get(
    "/generate_audio_url/{filename}", 
    tags=["üìÅ Fichiers"],
    summary="G√©n√©ration d'URL audio",
    description="G√©n√®re l'URL d'acc√®s pour un segment audio sp√©cifique"
)
def generate_audio_url(filename: str):
    return {"url": f"{server_url}/segment_audio/{filename}"}

# Endpoint pour servir les segments audio
@app.get(
    "/segment_audio/{filename}", 
    tags=["üìÅ Fichiers"],
    summary="T√©l√©chargement segment audio",
    description="T√©l√©charge directement un segment audio g√©n√©r√© lors de la transcription"
)
async def get_segment_audio(filename: str):
    # Utiliser le r√©pertoire temporaire syst√®me de mani√®re cross-platform
    file_path = os.path.join(tempfile.gettempdir(), filename)
    if os.path.exists(file_path):
        return FileResponse(file_path)
    else:
        return {"error": "Fichier non trouv√©"}

@app.post(
    "/transcribe_simple/", 
    tags=["üéµ Audio"],
    summary="üîß API Simple - Diarisation + Transcription",
    description="**Mode 1/3** - API pure pour int√©grations tierces. Traitement complet avec diarisation et transcription (accessible uniquement via Swagger/API, sans interface utilisateur)"
)
async def transcribe_simple(file: UploadFile = File(...)):
    """
    Mode Simple : API uniquement pour int√©grations tierces
    - Diarisation compl√®te (s√©paration des locuteurs)
    - Transcription haute qualit√©
    - Retour JSON structur√©
    - Aucune interface utilisateur (Swagger seulement)
    """
    logging.info("=== D√âBUT TRANSCRIBE_SIMPLE (API ONLY) ===")
    logging.info(f"üìÅ Fichier API re√ßu: {file.filename}, Type: {file.content_type}")
    
    # V√©rifier et initialiser les mod√®les STT si n√©cessaire
    global Transcriber_Whisper, diarization_model
    if Transcriber_Whisper is None or diarization_model is None:
        logging.info("Initialisation des mod√®les STT pour transcribe_simple...")
        load_core_models()  # Charger seulement les mod√®les STT (Whisper + diarisation)
        
        # V√©rifier que l'initialisation a r√©ussi
        if Transcriber_Whisper is None or diarization_model is None:
            logging.error("√âchec de l'initialisation des mod√®les STT")
            raise HTTPException(status_code=500, detail="Impossible d'initialiser les mod√®les de transcription. V√©rifiez les logs serveur.")
    
    async with async_temp_manager_context("transcribe_simple") as temp_manager:
        # Lire le fichier upload√©
        file_data = await file.read()
        file_path = temp_manager.create_temp_file(file.filename, file_data)

        # D√©tection de l'extension du fichier
        file_extension = os.path.splitext(file.filename)[1].lower()
        logging.info(f"üéØ Extension d√©tect√©e: {file_extension}")

        # Traitement audio/vid√©o
        if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
            logging.info(f"üéµ Fichier audio - traitement direct")
            audio = AudioSegment.from_file(file_path)
        elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
            logging.info(f"üé¨ Fichier vid√©o - extraction audio")
            VideoFileClip(file_path)
            audio = AudioSegment.from_file(file_path, format=file_extension)
        else:
            logging.error(f"‚ùå Format non support√©: {file_extension}")
            raise HTTPException(status_code=400, detail=f"Format de fichier non support√©: {file_extension}")

        # Normalisation audio
        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(16000)
        
        # Cr√©er un chemin pour le fichier audio converti
        audio_path = temp_manager.get_temp_path_with_suffix(".wav")
        audio.export(audio_path, format="wav")
        logging.info(f"‚úÖ Audio normalis√© sauvegard√©: {audio_path}")
        
        # √âtape 1 : Diarisation (s√©paration des locuteurs)
        logging.info("üéØ === D√âBUT DIARISATION ===")
        with ProgressHook() as hook:
            diarization = diarization_model(audio_path, hook=hook)
        logging.info("‚úÖ Diarisation termin√©e")

        # √âtape 2 : Transcription par segment
        logging.info("üìù === D√âBUT TRANSCRIPTION ===")
        segments = []

        for turn, _, speaker in diarization.itertracks(yield_label=True):
            # Extraire le segment audio correspondant au speaker
            start_ms = int(turn.start * 1000)  # Convertir en millisecondes
            end_ms = int(turn.end * 1000)
            segment_audio = audio[start_ms:end_ms]

            # Sauvegarder le segment temporairement pour Whisper
            segment_path = temp_manager.get_temp_path_with_suffix(".wav")
            segment_audio.export(segment_path, format="wav")

            # Transcrire ce segment avec Whisper
            transcription = Transcriber_Whisper(
                segment_path, 
                return_timestamps="word",
                generate_kwargs={"task": current_settings.get("task", "transcribe")}
            )
            
            # Ajouter le segment transcrit
            segments.append({
                "speaker": speaker,
                "text": transcription,
                "start_time": turn.start,
                "end_time": turn.end
            })
            logging.info(f"‚úÖ Transcrit {speaker}: {turn.start:.1f}s-{turn.end:.1f}s")

        logging.info(f"üéâ API Simple termin√© - {len(segments)} segments transcrits")
        
        # Retour structur√© pour l'API
        return {
            "mode": "simple_api",
            "status": "completed",
            "file_info": {
                "filename": file.filename,
                "format": file_extension,
                "duration_seconds": len(audio) / 1000.0
            },
            "diarization": {
                "total_speakers": len(set([seg["speaker"] for seg in segments])),
                "total_segments": len(segments)
            },
            "transcriptions": segments,
            "settings_used": {
                "model": current_settings.get("model", "default"),
                "task": current_settings.get("task", "transcribe"),
                "language": current_settings.get("lang", "auto")
            }
        }

@app.post(
    "/transcribe_file/", 
    tags=["üéµ Audio"],
    summary="üîÑ Interface Frontend - Traitement Standard", 
    description="**Mode 2/3** - Traitement complet audio/vid√©o avec diarisation et transcription pour interface utilisateur (sans streaming temps r√©el)"
)
async def transcribe_file(file: UploadFile = File(...)):
    """
    Mode Frontend Standard : Pour l'interface utilisateur actuelle
    - Diarisation compl√®te (s√©paration des locuteurs)
    - Transcription haute qualit√©
    - Retour optimis√© pour le frontend Vue.js
    - Compatible avec l'interface utilisateur existante
    """
    # V√©rifier et initialiser les mod√®les STT si n√©cessaire
    global Transcriber_Whisper, diarization_model
    if Transcriber_Whisper is None or diarization_model is None:
        logging.info("Initialisation des mod√®les STT pour transcribe_file...")
        load_core_models()  # Charger seulement les mod√®les STT (Whisper + diarisation)
        
        # V√©rifier que l'initialisation a r√©ussi
        if Transcriber_Whisper is None or diarization_model is None:
            logging.error("√âchec de l'initialisation des mod√®les STT")
            raise HTTPException(status_code=500, detail="Impossible d'initialiser les mod√®les de transcription. V√©rifiez les logs serveur.")
    
    async with async_temp_manager_context("transcribe_file") as temp_manager:
        # Lire le fichier upload√©
        file_data = await file.read()
        file_path = temp_manager.create_temp_file(file.filename, file_data)

        # D√©tection de l'extension du fichier
        file_extension = os.path.splitext(file.filename)[1].lower()

        # Si le fichier est un fichier audio (formats courants)
        if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
            # Charger le fichier audio avec Pydub
            audio = AudioSegment.from_file(file_path)

        elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
            VideoFileClip(file_path)
            audio = AudioSegment.from_file(file_path, format=file_extension)

        audio = audio.set_channels(1)
        audio = audio.set_frame_rate(16000)
        
        # Cr√©er un chemin pour le fichier audio converti
        audio_path = temp_manager.get_temp_path_with_suffix(".wav")
        audio.export(audio_path, format="wav")

        logging.info(f"Fichier {file.filename} sauvegard√© avec succ√®s.")
        
        # D√©marrer la diarisation
        logging.info("D√©marrage de la diarisation")

        # √âtape 1 : Diarisation
        with ProgressHook() as hook:
            logging.debug(f"Diarization d√©marr√©e")
            diarization = diarization_model(audio_path, hook=hook)
            logging.debug(f"Diarization termin√©e {diarization}")

        segments = []

        # total_turns = len(list(diarization.itertracks(yield_label=True))) 
        # for turn, _, speaker in tqdm(diarization.itertracks(yield_label=True), total=total_turns, desc="Processing turns"):
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            # √âtape 2 : Transcription pour chaque segment
            start_ms = int(turn.start * 1000)  # Convertir de secondes en millisecondes
            end_ms = int(turn.end * 1000)

            # Extraire le segment audio correspondant au speaker
            segment_audio = audio[start_ms:end_ms]

            # Sauvegarder le segment temporairement pour Whisper
            segment_path = temp_manager.get_temp_path_with_suffix(".wav")
            segment_audio.export(segment_path, format="wav")

            # Transcrire ce segment avec Whisper
            transcription = Transcriber_Whisper(
                segment_path, 
                # return_timestamps = True, 
                return_timestamps="word",
                generate_kwargs={"task": "transcribe"}
                )
            # √âtape 2 : Transcription pour chaque segment
            logging.info(f"Transcription du speaker {speaker} du segment de {turn.start} √† {turn.end} termin√©e")
            
            segments.append({
                "speaker": speaker,
                "text": transcription,
                "start_time": turn.start,
                "end_time": turn.end
            })
            logging.info("Transcription termin√©e.")

        return {"transcriptions": segments}
        # Le nettoyage est automatique avec async_temp_manager_context


import torchaudio
from denoiser import pretrained
from denoiser.dsp import convert_audio

# Charger le mod√®le pr√©-entra√Æn√© et le d√©placer sur le GPU
model_denoiser = pretrained.dns64().to(device)

# # Charger le mod√®le Silero VAD
# model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')
# get_speech_timestamps = utils['get_speech_timestamps'] if isinstance(utils, dict) else utils[0]
model_vad = load_silero_vad()

initial_maxlen = 1  # D√©but du buffer √† 1
target_maxlen = 5  # Valeur cible d√©finie pour le buffer

buffer = deque(maxlen=initial_maxlen)  # Buffer pour stocker 30 secondes

@app.websocket("/live_transcription/")
async def websocket_live_transcription(websocket: WebSocket):
    global buffer# Ajout de buffer comme variable globale pour √©viter l'erreur
    await websocket.accept()
    logging.debug("Client connect√©, en attente des donn√©es")

    try:
        while True:
            data = await websocket.receive_bytes()

            # Convertir les donn√©es re√ßues en AudioSegment
            audio_segment = AudioSegment(
                data=data,
                sample_width=2,  # 16 bits
                frame_rate=16000,
                channels=1
            )

            # Ajouter le segment audio au buffer
            buffer.append(audio_segment)

            # Augmente dynamiquement la taille de maxlen jusqu'√† atteindre target_maxlen
            if len(buffer) == buffer.maxlen and buffer.maxlen < target_maxlen:
                # Augmente maxlen du buffer
                buffer = deque(buffer, maxlen=buffer.maxlen + 1)
                logging.debug(f"buffer.maxlen: {buffer.maxlen}")

            # Calculer la dur√©e du chunk
            chunk_duration = audio_segment.duration_seconds
            logging.debug(f"Chunk duration: {chunk_duration} seconds")
            logging.debug(f"current_settings: {current_settings['task']}")
            print(f"Chunk duration: {chunk_duration} seconds")

            print(f"len(buffer)={len(buffer)}; buffer.maxlen={buffer.maxlen}")

            if len(buffer) == (buffer.maxlen):
                combined_audio = sum(buffer)  # Combine tous les AudioSegments en un seul

                # Cr√©er un fichier temporaire pour l'audio combin√©
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                    # Exporter combined_audio dans le fichier temporaire
                    combined_audio.export(tmp_file.name, format="wav")
                    tmp_file_path = tmp_file.name
                
                wav_vad = read_audio(tmp_file_path)

                speech_timestamps = get_speech_timestamps(wav_vad, model_vad)
                logging.debug(f"Speech Analysis: {speech_timestamps}")

                # Effectuer la d√©tection
                if speech_timestamps:
                    print("Speech d√©tect√© ...")

                    # Charger et pr√©parer le fichier audio
                    waveform, sample_rate = torchaudio.load(tmp_file_path)

                    # Convertir l'audio au bon format
                    waveform = convert_audio(waveform, sample_rate, model_denoiser.sample_rate, model_denoiser.chin)

                    # D√©placer les donn√©es sur le GPU
                    waveform = waveform.to(device)

                    # Appliquer la suppression de bruit
                    denoised_waveform = model_denoiser(waveform[None])[0]

                    # D√©placer les donn√©es nettoy√©es sur le CPU pour les sauvegarder (d√©tacher du graphe de calcul)
                    denoised_waveform = denoised_waveform.detach().cpu()

                    # Sauvegarder le fichier audio nettoy√©
                    torchaudio.save(tmp_file_path, denoised_waveform, model_denoiser.sample_rate)

                    if current_settings['task'] != "transcribe":
                        generate_kwargs={
                            "task": "translate",
                            "return_timestamps": False
                        }
                    else:
                        generate_kwargs={
                            "return_timestamps": False
                        }

                    # S'assurer que le mod√®le est charg√©
                    if Transcriber_Whisper_live is None:
                        load_models()

                    transcription_live = Transcriber_Whisper_live(
                        tmp_file_path,
                        generate_kwargs = generate_kwargs | generate_kwargs_live
                    )

                    print(f"Transcription: {transcription_live}")
                    logging.debug(f"Transcription: {transcription_live}")

                    # Envoyer les donn√©es au frontend
                    await websocket.send_json({
                        'chunk_duration': combined_audio.duration_seconds,
                        'transcription_live': transcription_live
                    })

                else:
                    print("Silence")
                    await websocket.send_json({
                        'chunk_duration': 0,
                        'transcription_live': {'text': "...\n"}
                    })

                # Supprimer le fichier temporaire apr√®s transcription
                os.remove(tmp_file_path)

    except WebSocketDisconnect:
        logging.debug("Client d√©connect√©")


def convert_tracks_to_json(tracks):
    # Liste pour stocker les segments format√©s
    formatted_segments = []

    # It√©rer sur les segments avec leurs labels
    for turn, _, speaker in tracks.itertracks(yield_label=True):
        segment = {"speaker": speaker, "start_time": turn.start, "end_time": turn.end}
        formatted_segments.append(segment)

    # Convertir la liste de segments en JSON
    # return json.dumps(formatted_segments)
    return formatted_segments

# Lib√©rer la m√©moire GPU une fois que vous avez termin√©
def release_whisper_memory():
    global whisper_pipeline
    
    try:
        del Transcriber_Whisper  # Supprime la r√©f√©rence au mod√®le
    except Exception as e:
        logging.error(f"Impossible de supprimer le mod√®le : {e}")

    torch.cuda.empty_cache()  # Vide le cache GPU pour lib√©rer la m√©moire
    print("Le mod√®le Whisper a √©t√© lib√©r√© de la m√©moire GPU.")


def extract_audio(file_path):
    """
    Extrait l'audio d'un fichier m√©dia (audio ou vid√©o) et retourne un AudioSegment.
    """
    # D√©tecter le type de fichier
    file_type = filetype.guess(file_path)
    
    if file_type is None:
        logging.error(f"Type de fichier non reconnu pour : {file_path}")
        return None
        
    logging.info(f"Type de fichier d√©tect√© : {file_type.mime}, Extension : {file_type.extension}")
    
    try:
        # Gestion des fichiers audio
        if file_type.mime.startswith('audio/'):
            logging.info(f"Fichier audio d√©tect√© : {file_type.mime}")
            return AudioSegment.from_file(file_path)
                
        # Gestion des fichiers vid√©o
        elif file_type.mime.startswith('video/'):
            logging.info(f"Fichier vid√©o d√©tect√© : {file_type.mime}")
            logging.info("Extraction Audio d√©marr√©e ...")
            
            # Cr√©er un fichier temporaire pour l'audio extrait
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
                temp_audio_path = temp_audio.name
                
            try:
                # Extraire l'audio de la vid√©o
                video = VideoFileClip(file_path)
                video.audio.write_audiofile(temp_audio_path, logger=None)
                video.close()
                
                # Charger l'audio extrait
                audio = AudioSegment.from_wav(temp_audio_path)
                
                # Nettoyer le fichier temporaire
                os.unlink(temp_audio_path)
                
                return audio
                
            except Exception as e:
                # Nettoyer en cas d'erreur
                if os.path.exists(temp_audio_path):
                    os.unlink(temp_audio_path)
                raise e
                
        else:
            logging.warning(f"Type de fichier non support√© : {file_type.mime}")
            return None
            
    except Exception as e:
        logging.error(f"Erreur lors de l'extraction audio : {e}")
        return None

def process_audio_chunk(data):
    # Cr√©er un AudioSegment √† partir des donn√©es brutes
    audio_chunk = AudioSegment.from_raw(io.BytesIO(data), sample_width=2, frame_rate=16000, channels=1)
    # Ici, vous pouvez ajouter des fonctionnalit√©s pour traiter ou analyser l'audio
    print("Processed audio chunk of size:", len(data))

    return audio_chunk


# Fonction pour ex√©cuter la commande `ollama run` et obtenir la r√©ponse du mod√®le
def run_chocolatine_model(prompt):
    # command_3b = [
    #     "ollama", "run", "jpacifico/chocolatine-3b",
    #     prompt
    # ]

    # command = [
    #     "ollama" "run" "chocolatine-128k:latest",
    #     prompt
    # ]

    # result = subprocess.run(command, capture_output=True, text=True)
    # logging.info("Command output:", result.stdout)  # Affiche la sortie pour v√©rification
    # logging.info("Command error:", result.stderr)  # Affiche les erreurs √©ventuelles
    
    messages = [
        {"role": "user", "content": prompt},
    ]
    
    response = Chocolatine(messages)
    
    return response

# Mod√®le de donn√©es pour la requ√™te POST
class QuestionWithTranscription(BaseModel):
    question: str
    transcription: str
    chat_model: str

def stream_output(process: subprocess.Popen) -> Generator[Tuple[str, str], None, None]:
    """
    G√©n√®re la sortie du processus en temps r√©el.
    Retourne des tuples (type, line) o√π type est 'stdout' ou 'stderr'
    """
    while True:
        # Lecture de stdout
        stdout_line = process.stdout.readline() if process.stdout else ''
        if stdout_line:
            yield 'stdout', stdout_line.strip()
        
        # Lecture de stderr
        stderr_line = process.stderr.readline() if process.stderr else ''
        if stderr_line:
            yield 'stderr', stderr_line.strip()
        
        # V√©rification si le processus est termin√©
        if process.poll() is not None and not stdout_line and not stderr_line:
            break


# Fonction pour ex√©cuter la commande en mode streaming
def run_chocolatine_streaming(prompt: str) -> Generator[str, None, None]:
    # Indique le d√©but du streaming
    yield "event: start\ndata: Le streaming a commenc√©\n\n"

      # Commande pour lancer le mod√®le
    command = ["ollama", "run", "jpacifico/chocolatine-3b", prompt]
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

    # Envoie chaque ligne produite par le mod√®le
    for line in process.stdout:
        yield f"{line.strip()}\n"  # Envoie chaque ligne de sortie avec un saut de ligne
        # await asyncio.sleep(0)  # Forcer l'envoi de chaque chunk

    # Termine le processus
    process.stdout.close()
    process.wait()

    # Indique la fin du streaming
    yield "event: end\ndata: Le streaming est termin√©\n\n"

# Fonction pour ex√©cuter la commande
def run_chocolatine(prompt: str) -> str:
    """Version synchrone optimis√©e de run_chocolatine"""
    logging.debug(f"D√©marrage run_chocolatine avec prompt: {prompt}")
    
    try:
        # Commande pour lancer le mod√®le
        command = ["ollama", "run", "jpacifico/chocolatine-3b", prompt]
        
        # Utilisation de check_output pour une capture simple et fiable
        result = subprocess.check_output(
            command,
            stderr=subprocess.PIPE,
            text=True,
            universal_newlines=True
        )
        
        logging.debug(f"R√©sultat brut re√ßu: {result}")
        return result.strip()

    except subprocess.CalledProcessError as e:
        logging.error(f"Erreur lors de l'ex√©cution: {e.stderr}")
        raise HTTPException(status_code=500, detail=str(e.stderr))
    except Exception as e:
        logging.error(f"Erreur inattendue: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Fonction pour ex√©cuter la commande en mode streaming avec gpt4o-mini
def run_gpt4o_mini_streaming(prompt: str) -> Generator[str, None, None]:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    try:
        # Cr√©ation de la r√©ponse
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=prompt,
            stream=False
        )
 
        logging.debug(f"R√©ponse compl√®te: {response}")
        return response
    
    except Exception as e:
        logging.error(f"Erreur GPT: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Route POST pour le streaming
@app.post(
    "/ask_question/", 
    tags=["ü§ñ Intelligence Artificielle"],
    summary="Questions sur transcription",
    description="Pose des questions sur une transcription avec les mod√®les IA (Chocolatine local ou GPT-4o-mini)"
)
async def ask_question(data: QuestionWithTranscription):
    # V√©rifier et charger le mod√®le IA si n√©cessaire (seulement pour Chocolatine)
    global Chocolatine
    if data.chat_model == "chocolatine" and Chocolatine is None:
        logging.info("Chargement du mod√®le IA (Chocolatine) pour ask_question...")
        load_ai_model()  # Charger seulement le mod√®le IA
        
        if Chocolatine is None:
            logging.error("√âchec du chargement du mod√®le Chocolatine")
            raise HTTPException(status_code=500, detail="Impossible de charger le mod√®le Chocolatine. V√©rifiez les logs serveur.")
    
    # Cr√©e le prompt pour le mod√®le √† partir de la question et de la transcription
    prompt_chocolatine = f""" 
Vous √™tes un assistant qui r√©pond aux questions et demandes bas√©es sur une transcription de conversation.

Voici les instructions √† suivre pour la r√©ponse √† apporter √† la demande:
    les r√©ponses doivent √™tre au format markdown, avec les points importants en **gras**, les extraits pris dans la conversation en *italique*
    la r√©ponse doit √™tre inf√©rieure √† 500 mots
    utilisez uniquement les informations contenues dans la transcription pour r√©pondre.

Voici ci dessous la transcription de la conversation entre crochets:
    [\n\n{data.transcription}]
    
Voici la demande de l'utilisateur : {data.question}
"""
    
    prompt_gpt = [ 
        {"role": "system", "content": "Vous √™tes l'assistant AKABI qui r√©pond aux questions bas√©es sur une transcription de conversation."},
        {"role": "user", "content": f"Voici la transcription de la conversation :\n\n{data.transcription}"},
        {"role": "assistant", "content": "Les r√©ponses doivent √™tre au format markdown, avec les points importants en **gras**, les extraits pris dans la conversation en *italique* et **AKABI** sera toujours √©crit en majuscule et en gras"},
        # {"role": "assistant", "content": "Les r√©ponses doivent √™tre format√©es en texte brut (donc sans symbole markdown) parce que l'affichage cot√© frontend ne g√®re pas le markdown, pour une lecture agr√©able avec des retours √† la ligne fr√©quents.  Utilisez uniquement les informations contenues dans la transcription pour r√©pondre"},
        {"role": "user", "content": f"Voici la demande de l'utilisateur : {data.question}"},
    ]

    logging.debug(f"Mod√®le utilis√©: {data.chat_model}")
    embedding_model, use_cases, use_case_files, index = setup_rag_pipeline()
    
    # S√©lection du prompt et de la fonction de streaming en fonction du mod√®le
    if data.chat_model == "chocolatine":
        if "AKABI" in (data.question).upper():  # pour ignorer la casse
            question_embedding = embedding_model.encode(data.question).astype("float32")
            # Recherche dans l'index FAISS
            _, indices = index.search(question_embedding.reshape(1, -1), k=5)
            relevant_texts = [use_cases[i] for i in indices[0]]

            # Pr√©parer le contexte pour la r√©ponse GPT
            context = " ".join(relevant_texts)
            prompt_chocolatine = f"{prompt_chocolatine}\nVoici √©galement des cas d'usage r√©alis√©s chez AKABI: {context}"


        logging.debug(f"Prompt envoy√© √† {data.chat_model}: {prompt_chocolatine}")

        response = run_chocolatine(prompt_chocolatine)

        json_response =  {"response": response}
        logging.debug(f"R√©ponse envoy√©e: {json_response}")
        
        return json_response

    else:
        if "AKABI" in (data.question).upper():  # pour ignorer la casse
            question_embedding = embedding_model.encode(data.question).astype("float32")
            # Recherche dans l'index FAISS
            _, indices = index.search(question_embedding.reshape(1, -1), k=5)
            relevant_texts = [use_cases[i] for i in indices[0]]
            
            # Pr√©parer le contexte pour la r√©ponse GPT
            context = " ".join(relevant_texts)
            prompt_gpt.append({"role": "system", "content": f"Voici des cas d'usage r√©alis√© chez AKABI: {context}"})
            # response = run_gpt4o_mini_streaming(prompt_gpt)

        response = run_gpt4o_mini_streaming(prompt_gpt)

        # R√©cup√©rer le contenu de la r√©ponse
        full_content = response.choices[0].message.content
        # json_response = jsonable_encoder(full_content.content)
        
        json_response =  {"response": full_content}
        logging.debug(f"R√©ponse envoy√©e: {json_response}")
        
        return json_response
        # return JSONResponse(content= json_response, media_type="application/json")

