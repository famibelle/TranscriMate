import asyncio
import json
import logging
import os
import shutil
import subprocess
import tempfile
import time
from contextlib import asynccontextmanager

import torch
from dotenv import load_dotenv
from fastapi import FastAPI, File, HTTPException, UploadFile, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pyannote.audio import Pipeline
from pyannote.audio.pipelines.utils.hook import ProgressHook
from pydantic import BaseModel, StrictStr, Field
from pydub import AudioSegment
from starlette.websockets import WebSocketDisconnect
from transformers import pipeline
import filetype
from moviepy.editor import VideoFileClip

from temp_manager import TempFileManager, async_temp_manager_context

# Configuration logging
logging.basicConfig(level=logging.INFO)

# Classe pour capturer la progression de pyannote
class ProgressCapture:
    def __init__(self):
        self.current_stage = ""
        self.progress = 0
        self.duration = 0
        self.callbacks = []
    
    def add_callback(self, callback):
        self.callbacks.append(callback)
    
    def update(self, stage, progress, duration):
        self.current_stage = stage
        self.progress = progress
        self.duration = duration
        for callback in self.callbacks:
            try:
                callback(stage, progress, duration)
            except Exception as e:
                logging.warning(f"Erreur callback progression: {e}")

# Instance globale pour la progression
progress_capture = ProgressCapture()

# Hook personnalis√© pour capturer la progression pyannote
class CustomProgressHook(ProgressHook):
    def __init__(self, callback_func=None):
        super().__init__()
        self.callback_func = callback_func
        self.start_time = time.time()
        
    def __call__(self, step_name: str, step_artifact, file=None, total=None, completed=None):
        # Appeler le hook parent avec la bonne signature
        result = super().__call__(step_name, step_artifact, file=file, total=total, completed=completed)
        
        # Calculer le pourcentage si on a des valeurs
        if total is not None and completed is not None and total > 0:
            progress_percent = int((completed / total) * 100)
        else:
            # Progression approximative selon l'√©tape
            progress_percent = 50 if step_artifact is not None else 0
            
        # Calculer le temps √©coul√© depuis le d√©but
        elapsed = time.time() - self.start_time
        
        # Mettre √† jour la capture globale
        progress_capture.update(step_name, progress_percent, elapsed)
        
        return result

# Configuration CUDA pour performance maximale
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True  # Optimise pour tailles fixes
torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention si disponible

# Optimisations m√©moire
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
# Supprimer les warnings verbeux
import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=UserWarning, module="torchaudio")  # TorchAudio warnings
warnings.filterwarnings("ignore", message=".*AudioMetaData has been deprecated.*")  # D√©pr√©ciation sp√©cifique
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # √âviter les warnings

# Chargement des variables d'environnement
load_dotenv()

# Variables globales pour les mod√®les
Transcriber_Whisper = None
Transcriber_Whisper_Light = None  # Mod√®le l√©ger pour le mode live
diarization_model = None
Chocolatine_pipeline = None

# Configuration des param√®tres par d√©faut
current_settings = {
    "task": "transcribe",
    "model": "openai/whisper-large-v3-turbo",
    "lang": "auto"
}

# Lifespan pour charger les mod√®les au d√©marrage
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await load_core_models()
    yield
    # Shutdown

app = FastAPI(
    title="TranscriMate - API de Transcription",
    description="API avec 3 modes : Simple API, Streaming, et Live WebSocket",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def load_core_models():
    """Charge les mod√®les Whisper, Pyannote et Chocolatine"""
    global Transcriber_Whisper, Transcriber_Whisper_Light, diarization_model, Chocolatine_pipeline
    
    logging.info("üîÑ Chargement des mod√®les...")
    
    try:
        # Chargement du mod√®le Whisper-base (l√©ger et multilingue)
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN non trouv√© dans les variables d'environnement")
        
        logging.info("üîÑ Chargement de Whisper-Large-v3-Turbo (haute qualit√©)...")
        Transcriber_Whisper = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-large-v3-turbo",  # Meilleure qualit√© disponible
            torch_dtype=torch.float16,  # FP16 pour optimisation GPU
            device="cuda" if torch.cuda.is_available() else "cpu",
            token=hf_token
        )
        
        # Chargement du mod√®le Whisper l√©ger pour le mode live
        logging.info("üîÑ Chargement de Whisper-base (l√©ger pour live)...")
        Transcriber_Whisper_Light = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-base",  # Mod√®le l√©ger pour temps r√©el
            torch_dtype=torch.float16,  # FP16 pour optimisation GPU
            device="cuda" if torch.cuda.is_available() else "cpu",
            token=hf_token
        )
        
        # Chargement du mod√®le de diarisation
        diarization_model = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        
        if torch.cuda.is_available():
            diarization_model = diarization_model.to(torch.device("cuda"))
        
        # Chargement du mod√®le Chocolatine
        logging.info("üîÑ Chargement du mod√®le Chocolatine...")
        try:
            pipeline_kwargs = {
                "task": "text-generation",
                "model": "jpacifico/Chocolatine-3B-Instruct-DPO-v1.2", 
                "trust_remote_code": True,
                "token": hf_token
            }
            
            # Configuration GPU optimis√©e (version compatible)
            if torch.cuda.is_available():
                pipeline_kwargs.update({
                    "device_map": "auto",  # Distribution automatique
                    "torch_dtype": torch.float16  # Half precision
                })
                logging.info("üöÄ Configuration GPU pour Chocolatine")
            else:
                pipeline_kwargs.update({
                    "device": "cpu",
                    "torch_dtype": torch.float32
                })
                logging.info("üíª Configuration CPU pour Chocolatine")
            
            Chocolatine_pipeline = pipeline(**pipeline_kwargs)
            logging.info("‚úÖ Mod√®le Chocolatine charg√© avec succ√®s")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Impossible de charger Chocolatine: {e}")
            Chocolatine_pipeline = None
        
        # V√©rification de la configuration FP16
        if torch.cuda.is_available():
            logging.info("‚úÖ Configuration GPU haute qualit√©:")
            logging.info(f"   üîπ CUDA Device: {torch.cuda.get_device_name(0)}")
            logging.info(f"   üîπ Whisper Large-v3-Turbo (principal): FP16 - Qualit√© maximale")
            logging.info(f"   üîπ Whisper Base (live): FP16 - Optimis√© temps r√©el")
            if Chocolatine_pipeline:
                logging.info(f"   üîπ Chocolatine: FP16 (torch.float16)")
        else:
            logging.info("üíª Configuration CPU: FP32 (torch.float32)")
        
        logging.info("‚úÖ Mod√®les charg√©s avec succ√®s")
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du chargement des mod√®les : {e}")
        raise

def convert_tracks_to_json(tracks):
    """Convertit les tracks de diarisation en JSON"""
    formatted_segments = []
    for turn, _, speaker in tracks.itertracks(yield_label=True):
        segment = {
            "speaker": speaker,
            "start_time": turn.start,
            "end_time": turn.end
        }
        formatted_segments.append(segment)
    return formatted_segments

def extract_and_prepare_audio(file_path: str, temp_manager: TempFileManager) -> str:
    """Extrait et pr√©pare l'audio depuis un fichier"""
    try:
        file_extension = os.path.splitext(file_path)[1].lower()
        logging.info(f"Traitement fichier: {file_path} (extension: {file_extension})")
        
        # Fichiers audio
        if file_extension in ['.mp3', '.wav', '.aac', '.ogg', '.flac', '.m4a']:
            logging.info(f"Fichier audio d√©tect√©: {file_extension}")
            audio = AudioSegment.from_file(file_path)
        
        # Fichiers vid√©o
        elif file_extension in ['.mp4', '.mov', '.3gp', '.mkv']:
            logging.info(f"Fichier vid√©o d√©tect√©: {file_extension}")
            audio = AudioSegment.from_file(file_path)
        
        else:
            raise HTTPException(
                status_code=400, 
                detail=f"Format non support√©: {file_extension}"
            )
            
        logging.info(f"Audio charg√©: dur√©e={len(audio)}ms, canaux={audio.channels}, fr√©quence={audio.frame_rate}Hz")
        
    except Exception as e:
        logging.error(f"Erreur lors du chargement du fichier {file_path}: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur traitement fichier: {str(e)}")
    
    # Conversion en mono 16kHz
    audio = audio.set_channels(1).set_frame_rate(16000)
    
    # Sauvegarde
    audio_path = temp_manager.get_temp_path_with_suffix(".wav")
    audio.export(audio_path, format="wav")
    
    return audio_path

# ==================== FONCTION CHOCOLATINE ====================

def run_chocolatine(prompt: str) -> str:
    """Fonction optimis√©e pour utiliser le mod√®le Chocolatine via Transformers"""
    global Chocolatine_pipeline
    
    if Chocolatine_pipeline is None:
        return "Le mod√®le Chocolatine n'est pas disponible. V√©rifiez les logs de d√©marrage."
    
    try:
        # Optimisation: limiter la longueur du prompt pour acc√©l√©ration
        if len(prompt) > 2000:
            prompt = prompt[:1800] + "... [texte tronqu√© pour optimisation]"
            
        logging.info(f"üç´ Chocolatine g√©n√©ration d√©marr√©e (prompt: {len(prompt)} chars)")
        start_time = time.time()
        
        # G√©n√©ration optimis√©e pour la vitesse
        response = Chocolatine_pipeline(
            prompt,  # Utiliser directement le prompt string
            max_new_tokens=150,  # R√©duire pour acc√©l√©rer
            temperature=0.3,     # Moins de randomness = plus rapide
            do_sample=True,
            top_p=0.9,          # Nucleus sampling pour qualit√©
            repetition_penalty=1.1,  # √âviter les r√©p√©titions
            pad_token_id=Chocolatine_pipeline.tokenizer.eos_token_id,
            eos_token_id=Chocolatine_pipeline.tokenizer.eos_token_id,
            return_full_text=False,  # Ne retourner que le texte g√©n√©r√©
            clean_up_tokenization_spaces=True
        )
        
        # Extraction optimis√©e du texte g√©n√©r√©
        elapsed_time = time.time() - start_time
        
        if response and len(response) > 0:
            generated_text = response[0]['generated_text']
            word_count = len(generated_text.split())
            tokens_per_sec = word_count / elapsed_time if elapsed_time > 0 else 0
            
            logging.info(f"‚úÖ Chocolatine termin√©: {elapsed_time:.2f}s, {word_count} mots, {tokens_per_sec:.1f} tok/s")
            # Avec return_full_text=False, on a directement la r√©ponse
            return generated_text.strip()
        else:
            logging.warning(f"‚ö†Ô∏è Chocolatine aucune r√©ponse en {elapsed_time:.2f}s")
            return "Aucune r√©ponse g√©n√©r√©e par Chocolatine."
            
    except Exception as e:
        elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
        logging.error(f"‚ùå Erreur Chocolatine apr√®s {elapsed_time:.2f}s: {e}")
        return f"Erreur lors de la g√©n√©ration: {str(e)}"

# ==================== MODE 1 : API SIMPLE ====================

class Settings(BaseModel):
    task: StrictStr = "transcribe"
    model: StrictStr = "openai/whisper-large-v3-turbo"
    lang: StrictStr = "auto"

@app.post(
    "/transcribe_simple/",
    tags=["üéØ Mode 1 - API Simple"],
    summary="Transcription compl√®te",
    description="Traitement complet : diarisation + transcription, retour JSON structur√©"
)
async def transcribe_simple(file: UploadFile = File(...)):
    """Mode 1 : API Simple - Transcription compl√®te avec diarisation"""
    
    # V√©rifier les mod√®les
    if Transcriber_Whisper is None or diarization_model is None:
        raise HTTPException(status_code=500, detail="Mod√®les non initialis√©s")
    
    async with async_temp_manager_context("transcribe_simple") as temp_manager:
        # Lecture et sauvegarde du fichier
        file_data = await file.read()
        file_path = temp_manager.create_temp_file(file.filename, file_data)
        
        # Extraction et pr√©paration audio
        audio_path = extract_and_prepare_audio(file_path, temp_manager)
        
        # Chargement audio pour segmentation
        audio = AudioSegment.from_wav(audio_path)
        
        # Diarisation avec progression test
        logging.info("üîÑ Diarisation en cours...")
        
        # Test simple : envoyer quelques progressions manuellement
        progress_capture.update("segmentation", 25, 1.0)
        progress_capture.update("segmentation", 50, 2.0)
        progress_capture.update("segmentation", 100, 3.0)
        progress_capture.update("speaker_counting", 100, 4.0)  
        progress_capture.update("embeddings", 50, 5.0)
        
        # Faire la diarisation r√©elle
        diarization = diarization_model(audio_path)
        
        # Finaliser la progression
        progress_capture.update("embeddings", 100, 8.0)
        
        # Sauvegarder le fichier audio complet pour acc√®s ult√©rieur
        full_audio_path = temp_manager.get_temp_path_with_suffix(".wav")
        audio.export(full_audio_path, format="wav")
        
        # Le fichier est d√©j√† dans le bon r√©pertoire temporaire
        audio_filename = os.path.basename(full_audio_path)
        
        # Transcription de chaque segment
        logging.info("üîÑ Transcription des segments...")
        segments = []
        
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            # Extraire le segment audio
            start_ms = int(turn.start * 1000)
            end_ms = int(turn.end * 1000)
            segment_audio = audio[start_ms:end_ms]
            
            # Sauvegarder temporairement
            segment_path = temp_manager.get_temp_path_with_suffix(".wav")
            segment_audio.export(segment_path, format="wav")
            
            # Transcrire
            transcription = Transcriber_Whisper(
                segment_path,
                return_timestamps=True,
                generate_kwargs={"task": current_settings["task"]}
            )
            
            segments.append({
                "speaker": speaker,
                "text": transcription["text"],
                "start_time": turn.start,
                "end_time": turn.end,
                "audio_url": f"/temp_audio/{audio_filename}",  # URL pour le fichier complet
                "segment_start": turn.start,  # Timestamps pour extraction c√¥t√© client
                "segment_end": turn.end
            })
        
        logging.info("‚úÖ Transcription simple termin√©e")
        return {
            "mode": "simple",
            "diarization": convert_tracks_to_json(diarization),
            "transcriptions": segments,
            "full_audio_url": f"/temp_audio/{audio_filename}"  # URL du fichier complet
        }

# ==================== MODE 2 : STREAMING ====================

@app.post(
    "/transcribe_streaming/",
    tags=["üåä Mode 2 - Streaming"],
    summary="Transcription en streaming",
    description="Traitement progressif avec retour en temps r√©el (Server-Sent Events)"
)
async def transcribe_streaming(file: UploadFile = File(...)):
    """Mode 2 : Streaming - Traitement progressif avec SSE"""
    
    # V√©rifier les mod√®les
    if Transcriber_Whisper is None or diarization_model is None:
        raise HTTPException(status_code=500, detail="Mod√®les non initialis√©s")
    
    # Lire le fichier avant le g√©n√©rateur pour √©viter les probl√®mes de connexion
    file_data = await file.read()
    filename = file.filename
    
    async def streaming_generator():
        async with async_temp_manager_context("transcribe_streaming") as temp_manager:
            try:
                # √âtape 1 : Pr√©paration
                yield f"data: {json.dumps({'status': 'started', 'message': 'D√©marrage du traitement'})}\n\n"
                
                file_path = temp_manager.create_temp_file(filename, file_data)
                audio_path = extract_and_prepare_audio(file_path, temp_manager)
                audio = AudioSegment.from_wav(audio_path)
                
                yield f"data: {json.dumps({'status': 'audio_ready', 'message': 'Audio pr√©par√©'})}\n\n"
                
                # √âtape 2 : Diarisation
                yield f"data: {json.dumps({'status': 'diarization_start', 'message': 'Diarisation en cours...'})}\n\n"
                
                try:
                    # Progression simul√©e pour streaming
                    async def simulate_diarization_progress():
                        stages = ['segmentation', 'speaker_counting', 'embeddings']
                        durations = [2, 1, 6]
                        
                        for i, stage in enumerate(stages):
                            start_time = time.time()
                            for progress in range(0, 101, 20):
                                elapsed = time.time() - start_time
                                progress_capture.update(stage, progress, elapsed)
                                await asyncio.sleep(durations[i] / 5)
                    
                    # Lancer progression en arri√®re-plan
                    progress_task = asyncio.create_task(simulate_diarization_progress())
                    
                    # Diarisation r√©elle
                    diarization = diarization_model(audio_path)
                    
                    # Arr√™ter progression
                    progress_task.cancel()
                    
                    diarization_json = convert_tracks_to_json(diarization)
                    yield f"data: {json.dumps({'status': 'diarization_done', 'diarization': diarization_json})}\n\n"
                except Exception as diar_error:
                    logging.error(f"Erreur diarisation streaming: {diar_error}")
                    yield f"data: {json.dumps({'status': 'error', 'message': f'Erreur diarisation: {str(diar_error)}'})}\n\n"
                    return
                
                # √âtape 3 : Transcription progressive
                try:
                    # Convertir en liste pour √©viter les probl√®mes d'it√©rateur
                    segments_list = list(diarization.itertracks(yield_label=True))
                    total_segments = len(segments_list)
                    segment_count = 0
                    
                    for turn, _, speaker in segments_list:
                        try:
                            segment_count += 1
                            
                            # Progress
                            progress = (segment_count / total_segments) * 100 if total_segments > 0 else 0
                            yield f"data: {json.dumps({'status': 'transcribing', 'progress': progress, 'segment': segment_count, 'total': total_segments})}\n\n"
                            
                            # Extraire segment
                            start_ms = int(turn.start * 1000)
                            end_ms = int(turn.end * 1000)
                            segment_audio = audio[start_ms:end_ms]
                            
                            segment_path = temp_manager.get_temp_path_with_suffix(".wav")
                            segment_audio.export(segment_path, format="wav")
                            
                            # Transcrire
                            transcription = Transcriber_Whisper(
                                segment_path,
                                return_timestamps=True,
                                generate_kwargs={"task": current_settings["task"]}
                            )
                            
                            # Envoyer r√©sultat du segment
                            segment_data = {
                                "speaker": speaker,
                                "text": transcription["text"],
                                "start_time": float(turn.start),
                                "end_time": float(turn.end),
                                "segment_number": segment_count
                            }
                            
                            yield f"data: {json.dumps({'status': 'segment_done', 'segment': segment_data})}\n\n"
                            await asyncio.sleep(0.01)  # Petit d√©lai pour le streaming
                            
                        except Exception as seg_error:
                            logging.error(f"Erreur segment {segment_count}: {seg_error}")
                            yield f"data: {json.dumps({'status': 'segment_error', 'segment': segment_count, 'message': str(seg_error)})}\n\n"
                            continue
                            
                except Exception as trans_error:
                    logging.error(f"Erreur transcription streaming: {trans_error}")
                    yield f"data: {json.dumps({'status': 'error', 'message': f'Erreur transcription: {str(trans_error)}'})}\n\n"
                    return
                
                # G√©n√©ration des URLs audio pour les segments
                yield f"data: {json.dumps({'status': 'generating_audio_urls', 'message': 'G√©n√©ration des URLs audio...'})}\n\n"
                
                # Sauvegarder le fichier audio complet pour acc√®s ult√©rieur
                full_audio_path = temp_manager.get_temp_path_with_suffix(".wav")
                audio.export(full_audio_path, format="wav")
                
                # Cr√©er le nom de fichier unique pour l'URL
                import uuid
                audio_filename = f"streaming_{uuid.uuid4().hex[:8]}.wav"
                
                # Copier vers le dossier temporaire accessible
                final_audio_path = f"backend/temp/{audio_filename}"
                os.makedirs("backend/temp", exist_ok=True)
                audio.export(final_audio_path, format="wav")
                
                # Envoyer les informations audio pour chaque segment
                audio_info = {
                    "full_audio_url": f"/temp_audio/{audio_filename}",
                    "segments_audio_info": []
                }
                
                # Pour chaque segment dans diarization, ajouter les infos audio
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    audio_info["segments_audio_info"].append({
                        "speaker": speaker,
                        "start_time": float(turn.start),
                        "end_time": float(turn.end),
                        "audio_url": f"/temp_audio/{audio_filename}"
                    })
                
                yield f"data: {json.dumps({'status': 'audio_urls_ready', 'audio_info': audio_info})}\n\n"
                
                # Fin
                yield f"data: {json.dumps({'status': 'completed', 'message': 'Transcription termin√©e'})}\n\n"
                
            except Exception as e:
                logging.error(f"Erreur streaming: {e}")
                yield f"data: {json.dumps({'status': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        streaming_generator(),
        media_type="text/plain",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
    )

# ==================== WEBSOCKET PROGRESSION ====================

@app.websocket("/progress/")
async def websocket_progress(websocket: WebSocket):
    """WebSocket pour suivre la progression des traitements"""
    await websocket.accept()
    logging.info("Client WebSocket connect√© pour progression")
    
    # Callback pour envoyer la progression
    async def send_progress(stage, progress, duration):
        try:
            await websocket.send_json({
                "type": "progress",
                "stage": stage,
                "progress": progress,
                "duration": duration,
                "timestamp": time.time()
            })
        except Exception as e:
            logging.warning(f"Erreur envoi progression: {e}")
    
    # Ajouter le callback √† la capture globale
    def sync_callback(stage, progress, duration):
        # Cr√©er une t√¢che asyncio dans l'event loop courant
        loop = asyncio.get_event_loop()
        if loop.is_running():
            asyncio.ensure_future(send_progress(stage, progress, duration))
        else:
            loop.run_until_complete(send_progress(stage, progress, duration))
    
    progress_capture.add_callback(sync_callback)
    
    try:
        # Garder la connexion ouverte
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        logging.info("Client WebSocket progression d√©connect√©")

# ==================== MODE 3 : WEBSOCKET LIVE ====================

@app.websocket("/live_transcription/")
async def live_transcription(websocket: WebSocket):
    """Mode 3 : WebSocket Live - Transcription temps r√©el depuis microphone"""
    
    await websocket.accept()
    logging.info("Client WebSocket connect√© pour transcription live")
    
    try:
        # V√©rifier les mod√®les (priorit√© au mod√®le l√©ger pour le live)
        whisper_model = Transcriber_Whisper_Light if Transcriber_Whisper_Light is not None else Transcriber_Whisper
        if whisper_model is None:
            await websocket.send_json({
                "status": "error",
                "message": "Aucun mod√®le Whisper disponible"
            })
            return
        
        # Confirmer la connexion avec informations sur le mod√®le
        model_info = "whisper-base-light" if whisper_model is Transcriber_Whisper_Light else "whisper-large"
        await websocket.send_json({
            "status": "connected",
            "message": f"WebSocket connect√© - Mode temps r√©el actif avec {model_info}",
            "model": model_info,
            "supports_multilingual": True,
            "buffer_duration_seconds": 2.0
        })
        
        # Buffer pour accumuler l'audio
        audio_buffer = []
        buffer_duration = 0.0
        target_duration = 2.0  # Transcrire toutes les 2 secondes
        
        while True:
            # Recevoir les donn√©es audio du client
            data = await websocket.receive_bytes()
            
            # Convertir les bytes en AudioSegment
            try:
                # Supposer que les donn√©es sont en Int16, mono, 16kHz
                audio_segment = AudioSegment(
                    data=data,
                    sample_width=2,  # 16 bits = 2 bytes
                    frame_rate=16000,
                    channels=1
                )
                
                # Ajouter au buffer
                audio_buffer.append(audio_segment)
                buffer_duration += audio_segment.duration_seconds
                
                # Si on a assez d'audio, transcrire
                if buffer_duration >= target_duration:
                    # Combiner tous les segments du buffer
                    combined_audio = sum(audio_buffer)
                    
                    # Cr√©er un fichier temporaire
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp_file:
                        combined_audio.export(tmp_file.name, format="wav")
                        
                        # Transcrire avec Whisper l√©ger
                        transcription = whisper_model(
                            tmp_file.name,
                            return_timestamps=True,  # Garder les timestamps pour plus d'infos
                            generate_kwargs={"task": current_settings["task"]}
                        )
                        
                        # Envoyer la transcription avec plus d'informations
                        response_data = {
                            "status": "transcription",
                            "text": transcription["text"].strip(),
                            "duration": round(buffer_duration, 2),
                            "chunk_duration": target_duration,
                            "model_used": "whisper-base-light" if whisper_model is Transcriber_Whisper_Light else "whisper-large",
                            "timestamp": round(asyncio.get_event_loop().time(), 2)
                        }
                        
                        # Ajouter les chunks si disponibles
                        if "chunks" in transcription:
                            response_data["chunks"] = transcription["chunks"]
                            
                        await websocket.send_json(response_data)
                        
                        # Le fichier sera automatiquement supprim√© √† la fin du 'with'
                    
                    # R√©initialiser le buffer
                    audio_buffer = []
                    buffer_duration = 0.0
                
                # Envoyer un accus√© de r√©ception p√©riodique
                elif len(audio_buffer) % 10 == 0:  # Toutes les 10 r√©ceptions
                    await websocket.send_json({
                        "status": "receiving",
                        "buffer_duration": buffer_duration,
                        "bytes_received": len(data)
                    })
                    
            except Exception as audio_error:
                logging.error(f"Erreur traitement audio: {audio_error}")
                await websocket.send_json({
                    "status": "error",
                    "message": f"Erreur traitement audio: {str(audio_error)}"
                })
            
    except WebSocketDisconnect:
        logging.info("Client WebSocket d√©connect√©")
    except Exception as e:
        logging.error(f"Erreur WebSocket: {e}")
        try:
            await websocket.send_json({"status": "error", "message": str(e)})
        except:
            pass

# ==================== ROUTES AUDIO ====================

# Stockage temporaire des fichiers audio (en production, utilisez Redis ou un cache appropri√©)
temp_audio_cache = {}

@app.get(
    "/temp_audio/{filename}",
    tags=["üéµ Audio"],
    summary="R√©cup√©ration d'un fichier audio temporaire",
    description="Retourne un fichier audio temporaire par son nom"
)
async def get_temp_audio(filename: str):
    """Retourne un fichier audio temporaire"""
    
    # Essayer d'abord dans backend/temp (pour les nouveaux fichiers streaming)
    local_temp_path = f"backend/temp/{filename}"
    if os.path.exists(local_temp_path):
        file_path = local_temp_path
    else:
        # Construire le chemin du fichier (dans /temp ou votre r√©pertoire temporaire)
        temp_dir = tempfile.gettempdir()
        file_path = os.path.join(temp_dir, filename)
        
        # V√©rifier que le fichier existe
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="Fichier audio non trouv√©")
    
    # Retourner le fichier
    return FileResponse(
        file_path,
        media_type="audio/wav",
        filename=filename,
        headers={"Cache-Control": "public, max-age=3600"}  # Cache 1 heure
    )

# ==================== ROUTES DE CONFIGURATION ====================

@app.post(
    "/settings/",
    tags=["‚öôÔ∏è Configuration"],
    summary="Mise √† jour des param√®tres",
    description="Configure les param√®tres de transcription"
)
def update_settings(settings: Settings):
    global current_settings
    current_settings = settings.model_dump()
    logging.info(f"Param√®tres mis √† jour: {current_settings}")
    return {"message": "Param√®tres mis √† jour avec succ√®s"}

@app.get(
    "/health/",
    tags=["üè• Sant√©"],
    summary="V√©rification de l'√©tat",
    description="V√©rifie l'√©tat des mod√®les et du syst√®me"
)
def health_check():
    models_status = {
        "whisper": Transcriber_Whisper is not None,
        "whisper_light": Transcriber_Whisper_Light is not None,
        "diarization": diarization_model is not None,
        "chocolatine": Chocolatine_pipeline is not None
    }
    
    # D√©terminer le statut global
    critical_models = ["whisper", "diarization"]  # Mod√®les critiques pour le fonctionnement
    all_critical_loaded = all(models_status[model] for model in critical_models)
    
    return {
        "status": "ready" if all_critical_loaded else "loading",
        "models_loaded": models_status,
        "critical_models_ready": all_critical_loaded,
        "cuda_available": torch.cuda.is_available(),
        "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        "settings": current_settings,
        "message": "Tous les mod√®les critiques sont charg√©s" if all_critical_loaded else "Chargement en cours..."
    }

# ==================== CHATBOT ====================

class QuestionRequest(BaseModel):
    question: str = Field(
        default="Pr√©sente-toi",
        description="La question √† poser √† l'IA",
        example="Pr√©sente-toi"
    )
    transcription: str = Field(
        default="Comment on appel un pain au chocolat dans le sud ?",
        description="Le texte de transcription sur lequel baser la r√©ponse",
        example="Comment on appel un pain au chocolat dans le sud ?"
    )
    chat_model: str = Field(
        default="chocolatine",
        description="Le mod√®le IA √† utiliser (chocolatine ou gpt-4)",
        example="chocolatine"
    )

    class Config:
        schema_extra = {
            "example": {
                "question": "Pr√©sente-toi",
                "transcription": "Comment on appel un pain au chocolat dans le sud ?",
                "chat_model": "chocolatine"
            }
        }

@app.post(
    "/ask_question/",
    tags=["ü§ñ Chatbot"],
    summary="R√©pondre √† une question",
    description="Utilise l'IA pour r√©pondre √† une question bas√©e sur la transcription"
)
async def ask_question(request: QuestionRequest):
    """Endpoint pour le chatbot - R√©ponse √† une question bas√©e sur la transcription"""
    
    try:
        # Construire le prompt complet
        prompt = f"""Voici une transcription d'une conversation:

{request.transcription}

Question: {request.question}

R√©ponds √† la question en te basant sur le contenu de la transcription. Sois pr√©cis et structure ta r√©ponse."""

        # Utiliser le mod√®le demand√©
        if request.chat_model == "chocolatine":
            response_text = run_chocolatine(prompt)
        elif request.chat_model == "gpt-4":
            # Pour GPT-4, on simule pour l'instant (n√©cessiterait une API key OpenAI)
            response_text = f"""[Simulation GPT-4] Bas√© sur la transcription fournie, voici ma r√©ponse √† votre question "{request.question}":

La transcription montre une conversation entre deux locuteurs discutant d'une journ√©e d'apprentissage du fran√ßais. 

Analyse d√©taill√©e:
- SPEAKER_01 semble √™tre un enseignant ou guide
- SPEAKER_00 est un √©tudiant partageant son exp√©rience
- La conversation porte sur des interactions multilingues et des activit√©s quotidiennes
- L'√©tudiant a eu une journ√©e productive avec des cours et des rencontres

La conversation r√©v√®le un environnement d'apprentissage positif et interactif."""
        else:
            response_text = f"Mod√®le '{request.chat_model}' non support√©. Mod√®les disponibles: chocolatine, gpt-4"

        return {
            "response": response_text,
            "model_used": request.chat_model,
            "status": "success"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors du traitement: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)